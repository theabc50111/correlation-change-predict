{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ce7c7-6cde-4c96-9c04-526e1394cb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pprint import pformat\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import MultipleLocator\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import silhouette_score, make_scorer\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import fbeta_score\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"/workspace/correlation-change-predict/ywt_library\")\n",
    "from data_module import gen_corr_dist_mat\n",
    "\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "data_config_path = current_dir / \"../config/data_config.yaml\"\n",
    "with open(data_config_path) as f:\n",
    "    data_cfg_yaml = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data_cfg_yaml))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# logger_list = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "# print(logger_list)\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.debug(pformat(data_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651a732-13ea-40cc-b683-99fcec1ee8c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ac923-2662-45b3-aab2-46cd99bef517",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data implement & output setting & testset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fbe48-0772-489a-acbc-c2c786050f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = Path('./results/')\n",
    "res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# setting of output files\n",
    "# save_corr_data = True\n",
    "# data implement setting\n",
    "# data_implement = \"SP500_20082017\"  # ['BITCOIN_NVDA', 'PAPER_EVA_1', 'PAPER_EVA_2', 'PAPER_EVA_3',\n",
    "                                                          # 'PAPER_EVA_4', 'PAPER_EVA_5', 'SP500_19972007', 'SP500_20082017',\n",
    "                                                          # 'SP500_20082017_CONSUMER_DISCRETIONARY', 'TEST_CASE', TETUAN_POWER', 'TW50_20082018']\n",
    "# etl set setting\n",
    "# etl_items_setting = \"-etl_all\"  # -etl_train|-etl_all\n",
    "# data split period setting, only suit for only settings of Korean paper\n",
    "# data_split_setting = \"-data_sp_test2\"\n",
    "# Decide how to calculate corr_ser\n",
    "corr_ser_clac_method = \"corr_ser_calc_regular\" # corr_ser_calc_regular|corr_ser_calc_abs\n",
    "# Decide composition of correlation_matrix\n",
    "# corr_mat_compo = \"sim\"\n",
    "\n",
    "#python data_module.py --data_implement SP500_20082017 --train_items_setting -train_all --data_split_setting -data_sp_test2 --graph_mat_compo sim --save_corr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf3224-3a83-4793-a105-c480f5bb151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(data_cfg[\"DATASETS\"][data_implement]['FILE_PATH'])\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns)  # all data\n",
    "train_set = data_cfg[\"DATASETS\"][data_implement]['TRAIN_SET']\n",
    "test_set = data_cfg['DATASETS'][data_implement]['TEST_SET'] if data_cfg['DATASETS'][data_implement].get('TEST_SET') else [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# test items implement settings\n",
    "items_implement = train_set if etl_items_setting == \"-etl_train\" else all_set\n",
    "logging.info(f\"===== len(etl set): {len(items_implement)} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + etl_items_setting\n",
    "fig_title = data_implement + etl_items_setting + data_split_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name}, fig_title basis:{fig_title} =====\")\n",
    "# display(dataset_df)\n",
    "\n",
    "# output folder settings\n",
    "corr_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-corr_data\"\n",
    "corr_data_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d7485-bf75-440f-9232-c10ceab8f40d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load or Create Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc41c1-4bfe-4db2-876a-980159ad8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = int(len(dataset_df)/data_gen_cfg[\"CORR_WINDOW\"])*data_gen_cfg[\"CORR_WINDOW\"]\n",
    "corr_ser_len_max = int((data_length-data_gen_cfg[\"CORR_WINDOW\"])/data_gen_cfg[\"CORR_STRIDE\"])\n",
    "max_data_div_start_add = 0  # In the Korea paper, each pair has 5 corr_series(due to diversifing train data).\n",
    "                            # BUT we only need to take one, so take 0 as arg.\n",
    "corr_ind = []\n",
    "\n",
    "# DEFAULT SETTING: data_gen_cfg[\"DATA_DIV_STRIDE\"] == 20, data_gen_cfg[\"CORR_WINDOW\"]==100, data_gen_cfg[\"CORR_STRIDE\"]==100\n",
    "data_end_init = corr_ser_len_max * data_gen_cfg[\"CORR_STRIDE\"]\n",
    "for i in range(0, max_data_div_start_add+1, data_gen_cfg[\"DATA_DIV_STRIDE\"]):\n",
    "    corr_ind.extend(list(range(data_gen_cfg[\"CORR_WINDOW\"]-1+i, data_end_init+bool(i)*data_gen_cfg[\"CORR_STRIDE\"], data_gen_cfg[\"CORR_STRIDE\"])))  # only suit for settings of paper\n",
    "\n",
    "train_df_path = corr_data_dir/f\"{output_file_name}-corr_train.csv\"\n",
    "dev_df_path = corr_data_dir/f\"{output_file_name}-corr_dev.csv\"\n",
    "test1_df_path = corr_data_dir/f\"{output_file_name}-corr_test1.csv\"\n",
    "test2_df_path = corr_data_dir/f\"{output_file_name}-corr_test2.csv\"\n",
    "all_corr_df_paths = dict(zip([\"train_df\", \"dev_df\", \"test1_df\", \"test2_df\"],\n",
    "                             [train_df_path, dev_df_path, test1_df_path, test2_df_path]))\n",
    "if all([df_path.exists() for df_path in all_corr_df_paths.values()]):\n",
    "    corr_datasets = [pd.read_csv(df_path, index_col=[\"items\"]) for df_path in all_corr_df_paths.values()]\n",
    "else:\n",
    "    corr_datasets = data_generation.gen_train_data(items_implement, raw_data_df=dataset_df, corr_ser_len_max=corr_ser_len_max, corr_df_paths=all_corr_df_paths, corr_ind=corr_ind, max_data_div_start_add=max_data_div_start_add, save_file=save_corr_data)\n",
    "\n",
    "if data_split_setting == \"-data_sp_test2\":\n",
    "    corr_dataset = corr_datasets[3]\n",
    "    display(corr_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab48c13c-7752-4105-bd68-205b0763cbaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Calculate properties of Corrlelation series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aca586-6ce7-4ff8-bc64-fc1b07d996f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stl_decompn(corr_datasets[0].iloc[0,::], overview=True)\n",
    "if corr_ser_clac_method == \"corr_ser_calc_regular\":\n",
    "    corr_property_df_path = res_dir/f\"{output_file_name}{data_split_setting}-corr_series_property.csv\"\n",
    "    corr_property_df = calc_corr_ser_property(corr_dataset=corr_dataset, corr_property_df_path=corr_property_df_path)\n",
    "elif corr_ser_clac_method == \"corr_ser_calc_abs\":\n",
    "    # calculate corr_property_df with abs(corr_dataset)\n",
    "    corr_property_df_path = res_dir/f\"{output_file_name}{data_split_setting}-corr_series_abs_property.csv\"\n",
    "    corr_property_df = calc_corr_ser_property(corr_dataset=corr_dataset.abs(), corr_property_df_path=corr_property_df_path)\n",
    "\n",
    "logging.info(f\"Min of corr_ser_mean:{corr_property_df.loc[::,'corr_ser_mean'].min()}\")\n",
    "display(corr_property_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b7022-6869-4990-bf4d-49bd97b239db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314cf997-4986-4a35-b75c-f01fb2f659d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## calculate dissimilarity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226fa6a-9702-46f7-a425-a88d79342b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_mean = corr_property_df.loc[::,\"corr_ser_mean\"]\n",
    "distance_mat = gen_corr_dist_mat(corr_mean, dataset_df, out_mat_compo=corr_mat_compo)\n",
    "# test\n",
    "# test_stock_tickers = [\"ED\", \"BAC\", \"XEL\", \"MA\"]\n",
    "# test_distance_mat = distance_mat.loc[test_stock_tickers, test_stock_tickers]\n",
    "# display(test_distance_mat)  # comlpete: (ED, BAC), (XEL), (MA) -> (ED, BAC), (XEL, MA)  -> (ED, BAC, XEL, MA)\n",
    "#                             # single: (ED, BAC), (XEL), (MA) -> (ED, BAC, XEL), (MA)  -> (ED, BAC, XEL, MA)\n",
    "logging.info(f\"Min of distance_mat:{distance_mat.min()}\")\n",
    "display(distance_mat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd61189a-728e-44d1-8ecb-8e42b0ca92fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## calculate cluster label for each data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a073dbd-cae4-4c0e-b6ab-b39c05329fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_silhouette_label_freq_std(estimator: \"sklearn.cluster.<cluster_model>\", X: \"pd.DataFrame\", silhouette_score_ratio: int = 0.1, silhouette_metric: str = \"precomputed\") -> float:\n",
    "    estimator.fit(X)\n",
    "    cluster_labels = estimator.labels_\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X.index)\n",
    "    labels_symbol, label_freq = np.unique(cluster_labels, return_counts=True)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return silhouette_score_ratio * silhouette_score(X, cluster_labels, metric=silhouette_metric) + (1 - silhouette_score_ratio) * (1 / np.array(label_freq).std())\n",
    "\n",
    "\n",
    "def hrchy_clustering_distance_threshold_rs(X: \"pd.DataFrame\", data_mat_mode: str = \"precomputed\", verbose: int = 0):\n",
    "\n",
    "    param_dict = {\"n_clusters\": [None], \"affinity\": [data_mat_mode],\n",
    "                  \"linkage\": [\"single\", \"complete\", \"average\"],\n",
    "                  \"distance_threshold\": uniform(loc=0.55, scale=0.6)}\n",
    "    cv = [(slice(None), slice(None))]\n",
    "    hrchy_clustering_rs = RandomizedSearchCV(estimator=AgglomerativeClustering(), param_distributions=param_dict,\n",
    "                                             n_iter=100000, scoring=calc_silhouette_label_freq_std, cv=cv, n_jobs=-1)\n",
    "    hrchy_clustering_rs.fit(X)\n",
    "\n",
    "    if verbose==1:\n",
    "        print(f\"hrchy_clustering_rs.best_estimator_: {hrchy_clustering_rs.best_estimator_}\")\n",
    "        print(f\"hrchy_clustering_rs.best_params_: {hrchy_clustering_rs.best_params_}\")\n",
    "        print(f\"hrchy_clustering_rs.best_score_: {hrchy_clustering_rs.best_score_}\")\n",
    "        print(f\"hrchy_clustering_rs.best_estimator_.n_leaves_: {hrchy_clustering_rs.best_estimator_.n_leaves_}\")\n",
    "        print(f\"hrchy_clustering_rs.best_estimator_.n_clusters_: {hrchy_clustering_rs.best_estimator_.n_clusters_}\")\n",
    "        print(f\"np.unique(hrchy_clustering_rs.best_estimator_.labels_): {np.unique(hrchy_clustering_rs.best_estimator_.labels_, return_counts=True)}\")\n",
    "        print(f\"hrchy_clustering_rs.best_estimator_.labels_: {hrchy_clustering_rs.best_estimator_.labels_}\")\n",
    "        print(f\"hrchy_clustering_rs.n_features_in_: {hrchy_clustering_rs.n_features_in_}\")\n",
    "        print(f\"hrchy_clustering_rs.feature_names_in_: {hrchy_clustering_rs.feature_names_in_}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "    return hrchy_clustering_rs.best_estimator_\n",
    "\n",
    "\n",
    "def hrchy_clustering_n_cluster_gs(X: \"pd.DataFrame\", data_mat_mode: str = \"precomputed\", verbose: int = 0):\n",
    "\n",
    "    param_dict = {\"n_clusters\": range(2, 20), \"affinity\": [data_mat_mode],\n",
    "                  \"linkage\": [\"single\", \"complete\", \"average\"]}\n",
    "    cv = [(slice(None), slice(None))]\n",
    "    hrchy_clustering_gs = GridSearchCV(estimator=AgglomerativeClustering(), param_grid=param_dict,\n",
    "                                       scoring=calc_silhouette_label_freq_std, cv=cv, n_jobs=-1)\n",
    "    hrchy_clustering_gs.fit(X)\n",
    "\n",
    "    if verbose==1:\n",
    "        print(f\"hrchy_clustering_gs.best_estimator_: {hrchy_clustering_gs.best_estimator_}\")\n",
    "        print(f\"hrchy_clustering_gs.best_params_: {hrchy_clustering_gs.best_params_}\")\n",
    "        print(f\"hrchy_clustering_gs.best_score_: {hrchy_clustering_gs.best_score_}\")\n",
    "        print(f\"hrchy_clustering_gs.best_estimator_.n_leaves_: {hrchy_clustering_gs.best_estimator_.n_leaves_}\")\n",
    "        print(f\"hrchy_clustering_gs.best_estimator_.n_clusters_: {hrchy_clustering_gs.best_estimator_.n_clusters_}\")\n",
    "        print(f\"np.unique(hrchy_clustering_gs.best_estimator_.labels_): {np.unique(hrchy_clustering_gs.best_estimator_.labels_, return_counts=True)}\")\n",
    "        print(f\"hrchy_clustering_gs.best_estimator_.labels_: {hrchy_clustering_gs.best_estimator_.labels_}\")\n",
    "        print(f\"hrchy_clustering_gs.n_features_in_: {hrchy_clustering_gs.n_features_in_}\")\n",
    "        print(f\"hrchy_clustering_gs.feature_names_in_: {hrchy_clustering_gs.feature_names_in_}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "    return hrchy_clustering_gs.best_estimator_\n",
    "\n",
    "\n",
    "def obs_hrchy_cluster_instances(X: \"pd.DataFrame\", data_mat_mode: str = \"precomputed\", verbose: int = 1):\n",
    "\n",
    "    for n in range(2, 20):\n",
    "        hrchy_cluster = AgglomerativeClustering(n_clusters=n, linkage=\"complete\", affinity=data_mat_mode, compute_distances=True)\n",
    "        hrchy_cluster.fit(X)\n",
    "\n",
    "        if verbose==1:\n",
    "            print(f\"hrchy_cluste.n_clusters_: {hrchy_cluster.n_clusters_}\")\n",
    "            print(f\"hrchy_cluste.labels and whose number of instances: {np.unique(hrchy_cluster.labels_, return_counts=True)}\")\n",
    "            # print(f\"(ticker, cluster label): {list(zip(X.index, hrchy_cluster.labels_))}\")\n",
    "            # print(f\"The estimated number of connected components:{hrchy_cluster.n_connected_components_}\")\n",
    "            # print(f\"hrchy_cluster.n_leaves_: {hrchy_cluster.n_leaves_}\")\n",
    "            # print(f\"hrchy_cluster.n_features_in_: {hrchy_cluster.n_features_in_}\")\n",
    "            print(\"-\"*50)\n",
    "\n",
    "\n",
    "def hrchy_cluster_fixed_n_cluster(X: \"pd.DataFrame\", n: int, data_mat_mode: str = \"precomputed\", verbose: int = 1):\n",
    "\n",
    "    hrchy_cluster = AgglomerativeClustering(n_clusters=n, linkage=\"complete\", affinity=data_mat_mode)\n",
    "    hrchy_cluster.fit(X)\n",
    "\n",
    "    if verbose==1:\n",
    "        print(f\"hrchy_cluste.n_clusters_: {hrchy_cluster.n_clusters_}\")\n",
    "        print(f\"hrchy_cluste.labels and whose instances: {np.unique(hrchy_cluster.labels_, return_counts=True)}\")\n",
    "        print(f\"hrchy_cluster.n_leaves_: {hrchy_cluster.n_leaves_}\")\n",
    "        print(f\"hrchy_cluster.n_features_in_: {hrchy_cluster.n_features_in_}\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "    return hrchy_cluster\n",
    "\n",
    "\n",
    "# obs_hrchy_cluster_instances(distance_mat)\n",
    "fixed_n_cluster = 11  # Determin by observe result of obs_hrchy_cluster_instances()\n",
    "fixed_n_cluster_hrchy_cluster = hrchy_cluster_fixed_n_cluster(distance_mat, n=fixed_n_cluster)\n",
    "# distance_threshold_hrchy_cluster = hrchy_clustering_distance_threshold_rs(dissimilarity_mat, verbose=0)\n",
    "# n_cluster_hrchy_cluster = hrchy_clustering_n_cluster_gs(distance_mat, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63eab05-a4d7-416d-817d-118f83e0f32f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## plot cluster label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fbbb8-a060-4c7e-957d-ff8671151ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_labels_distribution(trained_cluster: \"sklearn.cluster.<cluster_model>\", cluster_name: str):\n",
    "    x_major_locator = MultipleLocator(1)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(x_major_locator)\n",
    "    plt.bar(np.unique(trained_cluster.labels_, return_counts=True)[0], np.unique(trained_cluster.labels_, return_counts=True)[1])\n",
    "    plt.grid()\n",
    "    plt.ylabel(\"instances in cluster\")\n",
    "    plt.xlabel(\"cluster label\")\n",
    "    plt.title(f\"{cluster_name}\\n {fig_title}\")\n",
    "    plt.show()  # findout elbow point\n",
    "    plt.close()\n",
    "    logging.info(f\"cluster of each point distribution: {np.unique(trained_cluster.labels_, return_counts=True)}\")\n",
    "\n",
    "plot_cluster_labels_distribution(fixed_n_cluster_hrchy_cluster, f\"Hirerarchy clustering with {fixed_n_cluster} n_clusters\")\n",
    "# plot_cluster_labels_distribution(n_cluster_hrchy_cluster, \"Hirerarchy clustering with n_clusters\")\n",
    "# plot_cluster_labels_distribution(distance_threshold_hrchy_cluster, \"Hirerarchy clustering with distance threshold\")\n",
    "# print(hrchy_cluster_labels_df.loc[hrchy_cluster_labels_df[\"hrchy_cluster_label\"]==4, \"items\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc31dae-4ad5-4974-be50-6e99cbfedd09",
   "metadata": {},
   "source": [
    "## output cluster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d684cf2-abc2-44dd-bc3f-6579a2a6bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_items_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-cluster\"\n",
    "cluster_items_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if fixed_n_cluster:\n",
    "    output_cluster = fixed_n_cluster_hrchy_cluster  # the value of output_cluster depend on performance which shows in plot cluster label distribution\n",
    "    output_cluster_name = f\"corr_mat_hrchy_{fixed_n_cluster}_cluster\"\n",
    "\n",
    "hrchy_cluster_labels_df = pd.DataFrame(output_cluster.labels_, index=distance_mat.index, columns=[f\"{output_cluster_name}_label\"]).reset_index()\n",
    "hrchy_cluster_labels_df.to_csv(cluster_items_dir/f\"{output_file_name}-{corr_ser_clac_method}-{output_cluster_name}.csv\")\n",
    "logging.info(f\"{output_file_name}-{corr_ser_clac_method}-{output_cluster_name}.csv has been save to {cluster_items_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa1eaf-3921-4f56-9fe9-169e19c50f67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# plot correlation coffecient distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8ab23-bd5c-4e24-92b5-a61c4fcea458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = {'train_data--comb(150,2)': train_corr_series_concat, 'all_data--comb(445,2)': all_corr_series_concat, 'other_data--comb(295,2)': other_corr_series_concat}\n",
    "etl_types = [\"boxplot\", \"histogram\", \"qqplot\", \"Emprical Cumulative Density\"]\n",
    "fig, axes = plt.subplots(figsize=(20, 20),nrows=len(etl_types), ncols=len(datasets), sharex=False, sharey=False, dpi=100)\n",
    "\n",
    "for row, etl_type in enumerate(etl_types):\n",
    "    for col,dataset_key in enumerate(datasets):\n",
    "        # print(row, etl_type, col, dataset_key, datasets[dataset_key])\n",
    "        s = axes[row, col]\n",
    "        s.set_title(f\"{dataset_key}: \\n{etl_type}\", fontsize=24)\n",
    "        if etl_type==\"boxplot\":\n",
    "            s.boxplot(datasets[dataset_key], showmeans=True)\n",
    "        elif etl_type==\"histogram\":\n",
    "            s.hist(datasets[dataset_key], bins=[b/10 for b in range(-13,14)])\n",
    "        elif etl_type==\"qqplot\":\n",
    "            percents = [0.001, 0.2, 0.5, 0.8, 0.999]\n",
    "            #x,y = [norm.ppf(p) for p in percents], [np.quantile(train_corr_series_concat, p) for p in percents]\n",
    "            x,y = [norm.ppf(p) for p in percents], [np.quantile(datasets[dataset_key], p) for p in percents]\n",
    "            sm.qqplot(datasets[dataset_key], line='q', ax=s)\n",
    "            s.scatter(x,y, c='m', marker='x', s=300)\n",
    "        elif etl_type==\"Emprical Cumulative Density\":\n",
    "            pd.Series(datasets[dataset_key]).value_counts().sort_index().cumsum().plot(ax=s)\n",
    "\n",
    "# 分開, 避免子圖標籤互相重疊\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./results/dataset_exploration.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5bdbb5-58e5-4529-bd8d-781395f18015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[dataset_key, datasets[dataset_key].std()] for dataset_key in datasets], \n",
    "                  columns=['Dataset', 'Standard deviation'])\n",
    "ax = sns.barplot(x='Dataset', y='Standard deviation', data=df)\n",
    "ax.set_title('std of correlation')\n",
    "ax.set(ylim=[0.47, 0.475])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.xticks(rotation=60)\n",
    "plt.savefig(\"./results/dataset_exploration_2.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924fcc9-455d-42b1-b08f-ced180ce03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_corr_series_concat)\n",
    "# plt.hist(train_corr_series, bins=[b/10 for b in range(-13,14)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e0835-ae6c-4cca-969b-e2e33f608f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr_series_df = gen_corr_series(None, \"train_dataset.csv\", from_file=True, concat_all=False)\n",
    "all_corr_series_df = gen_corr_series(None, \"445_dataset.csv\", from_file=True, concat_all=False)\n",
    "other_corr_series_df = gen_corr_series(None, \"295_dataset.csv\", from_file=True, concat_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5754944-5cd5-41f1-8c37-e607ba544e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = {'train_data--comb(150,2)': train_corr_series_df, 'all_data--comb(445,2)': all_corr_series_df, 'other_data--comb(295,2)': other_corr_series_df}\n",
    "etl_types = [\"boxplot\", \"histogram\"]\n",
    "static_types = [\"mean\", \"std\"]\n",
    "fig, axes = plt.subplots(figsize=(30, 30),nrows=len(list(product(etl_types, static_types))), ncols=len(datasets), sharex=False, sharey=False, dpi=100)\n",
    "\n",
    "for row, (etl_type, static_type) in enumerate(product(etl_types, static_types)):\n",
    "    for col,dataset_key in enumerate(datasets):\n",
    "        s = axes[row, col]\n",
    "        s.set_title(f\"{dataset_key}: \\n{etl_type}_{static_type}\", fontsize=24)\n",
    "        if etl_type==\"boxplot\":\n",
    "            s.boxplot(datasets[dataset_key].iloc[:, ::5].describe().loc[static_type,:], showmeans=True)\n",
    "        elif etl_type==\"histogram\":\n",
    "            s.hist(datasets[dataset_key].iloc[:, ::5].describe().loc[static_type,:], bins=[b/10 for b in range(-13,14)])\n",
    "\n",
    "fig.suptitle(f\"Each correlation_series static property _20220718\", fontsize=24)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# 分開, 避免子圖標籤互相重疊\n",
    "# plt.tight_layout()\n",
    "plt.savefig(\"./results/dataset_exploration_3.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3beb76-3499-4905-96cc-4d208947a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_corr_series_df)\n",
    "display(train_corr_series_df.iloc[:,::5])\n",
    "display(train_corr_series_df.iloc[:,::5].describe())\n",
    "display(train_corr_series_df.iloc[:,::5].describe().loc['std',:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
