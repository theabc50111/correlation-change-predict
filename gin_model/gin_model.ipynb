{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333f12fa-b283-4785-9ff3-f92ee7611217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{'CORR_STRIDE': 5, 'CORR_WINDOW': 5, 'DATA_DIV_STRIDE': 20, 'MAX_DATA_DIV_START_ADD': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.79 s (started: 2023-01-29 18:23:59 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch.nn import Linear, GRU, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"/workspace/correlation-change-predict/ywt_library\")\n",
    "import data_generation\n",
    "from data_generation import data_gen_cfg, gen_corr_dist_mat\n",
    "from stl_decompn import stl_decompn\n",
    "from corr_property import calc_corr_ser_property\n",
    "\n",
    "\n",
    "with open('../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# logger_list = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "# print(logger_list)\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.debug(pformat(data_cfg, indent=1, width=100, compact=True))\n",
    "logging.info(pformat(data_gen_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9618165-42eb-4b2b-b691-f18bbdb85b6c",
   "metadata": {},
   "source": [
    "## Data implement & output setting & testset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f62f14-8b0e-4362-8321-9379b91e7d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:===== file_name basis:sp500_20082017_corr_ser_reg_corr_mat_hrchy_11_cluster-train_train =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.08 ms (started: 2023-01-29 18:24:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# data implement setting\n",
    "data_implement = \"SP500_20082017_CORR_SER_REG_CORR_MAT_HRCHY_11_CLUSTER\"  # watch options by operate: print(data_cfg[\"DATASETS\"].keys())\n",
    "# train set setting\n",
    "train_items_setting = \"-train_train\"  # -train_train|-train_all\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + train_items_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")\n",
    "\n",
    "\n",
    "graph_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-graph_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db4a6e4e-e361-4b37-be10-8943c077c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 464 Âµs (started: 2023-01-29 18:24:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "mts_corr_ad_cfg = {\"tr_loader_batch\": 12,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"va_loader_batch\": 4,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"tt_loader_batch\": 4,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"gin_dim_h\": 32,\n",
    "                   \"gru_layers\": 1,\n",
    "                   \"gru_dim_out\": 32,\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904302d0-3d2b-4f4d-8996-1bbf468b3083",
   "metadata": {},
   "source": [
    "## Load Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1b5110-24fa-4739-b56a-b4f336f518d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:graph_arr.shape:(499, 66, 66)\n",
      "INFO:root:data.num_node_features: 1; data.num_edges: 4356; data.num_edge_features: 1; data.is_undirected: True; \n",
      "INFO:root:data.x.shape: torch.Size([66, 1]); data.y.shape: torch.Size([4356]); data.edge_index.shape: torch.Size([2, 4356]); data.edge_attr.shape: torch.Size([4356, 1])\n",
      "INFO:root:Training set   = 448 graphs\n",
      "INFO:root:Validation set = 25 graphs\n",
      "INFO:root:Test set       = 25 graphs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 157 ms (started: 2023-01-29 18:24:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "graph_arr = np.load(graph_data_dir/f\"corr_calc_reg-corr_graph.npy\")  # each graph consist of 66 node & 66^2 edges\n",
    "logging.info(f\"graph_arr.shape:{graph_arr.shape}\")\n",
    "graph_time_step = graph_arr.shape[0] - 1  # the graph of last \"t\" can't be used as train data\n",
    "node_attr = torch.tensor(np.zeros((graph_arr.shape[1], 1)), dtype=torch.float32)  # each node has only one attribute\n",
    "edge_index = torch.tensor(list(product(range(graph_arr.shape[1]), repeat=2)))\n",
    "dataset = []\n",
    "for g_t in range(graph_time_step):\n",
    "    edge_attr = torch.tensor(np.hstack(graph_arr[g_t]).reshape(-1, 1), dtype=torch.float32)\n",
    "    edge_attr_next_t = torch.tensor(np.hstack(graph_arr[g_t+1]))\n",
    "    data = Data(x=node_attr, y=edge_attr_next_t, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr)\n",
    "    dataset.append(data)\n",
    "else:\n",
    "    mts_corr_ad_cfg[\"num_node_features\"] = data.num_node_features\n",
    "    mts_corr_ad_cfg[\"dim_out\"] = data.y.shape[0]\n",
    "    logging.info(f\"data.num_node_features: {data.num_node_features}; data.num_edges: {data.num_edges}; data.num_edge_features: {data.num_edge_features}; data.is_undirected: {data.is_undirected()}; \")\n",
    "    logging.info(f\"data.x.shape: {data.x.shape}; data.y.shape: {data.y.shape}; data.edge_index.shape: {data.edge_index.shape}; data.edge_attr.shape: {data.edge_attr.shape}\")\n",
    "\n",
    "# Create training, validation, and test sets\n",
    "train_dataset = dataset[:int(len(dataset)*0.9)]\n",
    "val_dataset   = dataset[int(len(dataset)*0.9):int(len(dataset)*0.95)]\n",
    "test_dataset  = dataset[int(len(dataset)*0.95):]\n",
    "\n",
    "# Create mini-batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=mts_corr_ad_cfg[\"tr_loader_batch\"], shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=mts_corr_ad_cfg[\"va_loader_batch\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=mts_corr_ad_cfg[\"tt_loader_batch\"], shuffle=False)\n",
    "\n",
    "# show info\n",
    "logging.info(f'Training set   = {len(train_dataset)} graphs')\n",
    "logging.info(f'Validation set = {len(val_dataset)} graphs')\n",
    "logging.info(f'Test set       = {len(test_dataset)} graphs')\n",
    "logging.debug('\\nTrain loader:')\n",
    "for i, subgraph in enumerate(train_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph {i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "logging.debug('\\nValidation loader:')\n",
    "for i, subgraph in enumerate(val_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph{i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "logging.debug('\\nTest loader:')\n",
    "for i, subgraph in enumerate(test_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph{i}.num_graphs:{subgraph.num_graphs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77157a0c-3ff4-4e6a-a5e1-a32dfdc0b6af",
   "metadata": {},
   "source": [
    "## Multi-Dimension Time-Series Correlation Anomly Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e5e2b9-8ec8-4d7d-b670-f157597aa1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.27 ms (started: 2023-01-29 18:24:01 +00:00)\n"
     ]
    }
   ],
   "source": [
    "class MTSCorrAD(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    num_node_features: number of features per node in the graph, in this model every node has same size of features \n",
    "    gin_dim_h: output size of hidden layer of GINconv\n",
    "    gru_layers: Number of recurrent layers of GRU\n",
    "    gru_dim_out: The number of output size of GRU and features in the hidden state h of GRU\n",
    "    dim_out: The number of output size of MTSCorrAD model\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features:int, gin_dim_h:int, gru_layers:int, gru_dim_out:int, dim_out:int, **kwargs):\n",
    "        super(MTSCorrAD, self).__init__()\n",
    "        self.ginconv1 = GINConv(\n",
    "            Sequential(Linear(num_node_features, gin_dim_h),\n",
    "                       BatchNorm1d(gin_dim_h), ReLU(),\n",
    "                       Linear(gin_dim_h, gin_dim_h), ReLU()))\n",
    "        self.ginconv2 = GINConv(\n",
    "            Sequential(Linear(gin_dim_h, gin_dim_h),\n",
    "                       BatchNorm1d(gin_dim_h), ReLU(),\n",
    "                       Linear(gin_dim_h, gin_dim_h), ReLU()))\n",
    "        self.ginconv3 = GINConv(\n",
    "            Sequential(Linear(gin_dim_h, gin_dim_h),\n",
    "                       BatchNorm1d(gin_dim_h), ReLU(),\n",
    "                       Linear(gin_dim_h, gin_dim_h), ReLU()))\n",
    "        self.gru1 = GRU(gin_dim_h*3, gru_dim_out, gru_layers)  # the input size of GRU depend on the number of layers of GINconv\n",
    "        self.lin1 = Linear(gru_dim_out, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch_node_id):\n",
    "        # Inter-series modeling\n",
    "        # Node embeddings\n",
    "        h1 = self.ginconv1(x, edge_index)\n",
    "        h2 = self.ginconv2(h1, edge_index)\n",
    "        h3 = self.ginconv3(h2, edge_index)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h1 = global_add_pool(h1, batch_node_id)\n",
    "        h2 = global_add_pool(h2, batch_node_id)\n",
    "        h3 = global_add_pool(h3, batch_node_id)\n",
    "\n",
    "        # Concatenate graph embeddings\n",
    "        graph_embeds = torch.cat((h1, h2, h3), dim=1)  # the shape of graph_embeds: [batch_size, num_layers*gin_dim_h]\n",
    "\n",
    "        # Temporal Modeling\n",
    "        gru_output, gru_hn = self.gru1(graph_embeds)  # regarding batch_size as time-steps(sequence length) by using \"unbatched\" input\n",
    "        fc_output = self.lin1(gru_output[-1])  # gru_output[-1] => only take last time-step\n",
    "\n",
    "        return graph_embeds, fc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946d826a-0dc6-47dd-a74b-738bd7d09669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 38/38 [00:00<00:00, 95.85it/s] \n",
      "100% 38/38 [00:00<00:00, 160.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.63 s (started: 2023-01-29 18:24:01 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model =  MTSCorrAD(**mts_corr_ad_cfg).to(\"cuda\")\n",
    "criterion = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                  lr=0.01,\n",
    "                                  weight_decay=0.01)\n",
    "epochs = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs+1):\n",
    "    total_loss = 0\n",
    "    acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    # Train on batches\n",
    "    for data in tqdm(train_loader):\n",
    "        data.to(\"cuda\")\n",
    "        x, y = data.x, data.y[-mts_corr_ad_cfg[\"dim_out\"]:]\n",
    "        optimizer.zero_grad()\n",
    "        graph_embeds, out = model(x, data.edge_index, data.batch)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss / len(train_loader)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # acc += accuracy(out.argmax(dim=1), data.y) / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25312b08-c3e7-4da8-96ab-1733b5324d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
