{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f67d596-88ae-4393-bbb4-d4301d878057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.01 s (started: 2022-12-12 14:42:45 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pprint import pformat\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"/tf/correlation-coef-predict/ywt_library\")\n",
    "import data_generation\n",
    "from data_generation import data_gen_cfg\n",
    "from ywt_arima import arima_model, arima_err_logger_init\n",
    "\n",
    "with open('../../../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.debug(pformat(data_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4cd793-e4d8-43b3-b592-bad9cf4342f6",
   "metadata": {},
   "source": [
    "## Data implement & output setting & trainset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7765e10-2944-4621-b62f-6602a573387f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 611 µs (started: 2022-12-12 14:43:03 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# setting of output files\n",
    "save_corr_data = True\n",
    "save_lstm_resid_data = True\n",
    "# data implement setting\n",
    "data_implement = \"SP500_20082017\"  # watch options by operate: print(data_cfg[\"DATASETS\"].keys())\n",
    "# train set setting\n",
    "train_items_setting = \"-train_train\"  # -train_train|-train_all\n",
    "# data split  period setting, only suit for only settings of Korean paper\n",
    "data_split_settings = [\"-data_sp_train\", \"-data_sp_dev\", \"-data_sp_test1\", \"-data_sp_test2\", ]\n",
    "# lstm_hyper_params\n",
    "first_stage_lstm_hyper_param = \"-kS_hyper\"\n",
    "second_stage_lstm_hyper_param = \"-kS_hyper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eca3555-279b-42f5-88e3-fe8a7745242f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:===== len(train_set): 150, len(all_set): 446, len(test_set): 296 =====\n",
      "INFO:root:===== len(train set): 150 =====\n",
      "INFO:root:===== file_name basis:sp500_20082017-train_train =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 104 ms (started: 2022-12-12 14:43:04 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# data loading & implement setting\n",
    "dataset_df = pd.read_csv(data_cfg[\"DATASETS\"][data_implement]['FILE_PATH'])\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns)  # all data\n",
    "train_set = data_cfg[\"DATASETS\"][data_implement]['TRAIN_SET']\n",
    "test_set = data_cfg['DATASETS'][data_implement]['TEST_SET'] if data_cfg['DATASETS'][data_implement].get('TEST_SET') else [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# train items implement settings\n",
    "items_implement = train_set if train_items_setting == \"-train_train\" else all_set\n",
    "logging.info(f\"===== len(train set): {len(items_implement)} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + train_items_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")\n",
    "# display(dataset_df)\n",
    "\n",
    "# output folder settings\n",
    "corr_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-corr_data\"\n",
    "first_stage_lstm_model_dir = Path('./save_models/first_stage_lstm_weights')\n",
    "first_stage_lstm_log_dir = Path('./save_models/first_stage_lstm_train_logs')\n",
    "first_stage_lstm_result_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-first_stage_lstm_res\"\n",
    "second_stage_lstm_model_dir = Path('./save_models/second_stage_lstm_weights')\n",
    "second_stage_lstm_log_dir = Path('./save_models/second_stage_lstm_train_logs')\n",
    "res_dir = Path('./results/')\n",
    "corr_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "first_stage_lstm_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "first_stage_lstm_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "first_stage_lstm_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "second_stage_lstm_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "second_stage_lstm_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f530b-2393-4a1d-82c0-38382b1d6133",
   "metadata": {},
   "source": [
    "## Load or Create Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916be840-68c4-4b6a-b188-5a29cc991d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = int(len(dataset_df)/data_gen_cfg[\"CORR_WINDOW\"])*data_gen_cfg[\"CORR_WINDOW\"]\n",
    "corr_ser_len_max = int((data_length-data_gen_cfg[\"CORR_WINDOW\"])/data_gen_cfg[\"CORR_STRIDE\"])\n",
    "\n",
    "train_df_path = corr_data_dir/f\"{output_file_name}-corr_train.csv\"\n",
    "dev_df_path = corr_data_dir/f\"{output_file_name}-corr_dev.csv\"\n",
    "test1_df_path = corr_data_dir/f\"{output_file_name}-corr_test1.csv\"\n",
    "test2_df_path = corr_data_dir/f\"{output_file_name}-corr_test2.csv\"\n",
    "all_corr_df_paths = dict(zip([\"train_df\", \"dev_df\", \"test1_df\", \"test2_df\"],\n",
    "                             [train_df_path, dev_df_path, test1_df_path, test2_df_path]))\n",
    "if all([df_path.exists() for df_path in all_corr_df_paths.values()]):\n",
    "    corr_datasets = [pd.read_csv(df_path, index_col=[\"items\"]) for df_path in all_corr_df_paths.values()]\n",
    "else:\n",
    "    corr_datasets = data_generation.gen_train_data(items_implement, raw_data_df=dataset_df, corr_df_paths=all_corr_df_paths, corr_ser_len_max=corr_ser_len_max, save_file=save_corr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM model for first stage prediciton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d270eaf2-c32a-404c-a03f-cc52a0349d1c",
   "metadata": {},
   "source": [
    "## settings of input data of first stage LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8dbc61-7807-4cd3-b3d6-da55754fc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stage_lstm_X_len = corr_datasets[0].shape[1]-1\n",
    "first_stage_lstm_Y_len = corr_datasets[0].shape[1]\n",
    "first_stage_lstm_train_X = corr_datasets[0].iloc[::, :first_stage_lstm_X_len].values.reshape(-1, first_stage_lstm_X_len, 1)\n",
    "first_stage_lstm_train_Y = corr_datasets[0].values.reshape(-1, first_stage_lstm_Y_len, 1)\n",
    "first_stage_lstm_dev_X = corr_datasets[1].iloc[::, :first_stage_lstm_X_len].values.reshape(-1, first_stage_lstm_X_len, 1)\n",
    "first_stage_lstm_dev_Y = corr_datasets[1].values.reshape(-1, first_stage_lstm_Y_len, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12a4d5-d110-4a3f-9c55-b85a7f8d5a1e",
   "metadata": {},
   "source": [
    "## settings of first stage LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8a573b-d7e3-402d-b65c-5bdeaa85062c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 381 ms (started: 2022-12-12 14:43:10 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:43:10.473227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.477196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.477412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.477975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 14:43:10.478757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.478976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.479173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.799110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.799272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.799397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 14:43:10.799498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 298 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "first_stage_lstm_model_log = TensorBoard(log_dir=first_stage_lstm_log_dir)\n",
    "first_stage_lstm_model_earlystop = EarlyStopping(patience=500, monitor=\"val_loss\")\n",
    "first_stage_lstm_save_model = ModelCheckpoint(Path(first_stage_lstm_model_dir)/\"epoch{epoch}_{val_loss:.5f}.h5\",\n",
    "                                             monitor='val_loss', verbose=1, mode='min', save_best_only=False)\n",
    "first_stage_lstm_callbacks_list = [first_stage_lstm_model_log, first_stage_lstm_model_earlystop, first_stage_lstm_save_model]\n",
    "first_stage_lstm_max_epoch = 5000\n",
    "first_stage_lstm_batch_size = 64\n",
    "first_stage_lstm_metrics = ['mse', 'mae']\n",
    "\n",
    "if first_stage_lstm_hyper_param == \"-kS_hyper\":\n",
    "    lstm_layer = LSTM(units=10, kernel_regularizer=l1_l2(0.2, 0.0), bias_regularizer=l1_l2(0.2, 0.0), activation=\"tanh\", dropout=0.1, name=f\"lstm{first_stage_lstm_hyper_param}\")  # LSTM hyper params from 【Something Old, Something New — A Hybrid Approach with ARIMA and LSTM to Increase Portfolio Stability】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a7e5c8-9735-4ab7-8613-770ff3c38c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_lstm_first_stage-lstm1_fc1-kS_hyper\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " lstm-kS_hyper (LSTM)        (None, 10)                480       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 21)                231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 711\n",
      "Trainable params: 711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 257 ms (started: 2022-12-12 14:43:21 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) *2)\n",
    "\n",
    "\n",
    "def build_first_stage_lstm():\n",
    "    inputs = Input(shape=(20, 1))\n",
    "    lstm_1 = lstm_layer(inputs)\n",
    "    outputs = Dense(units=21, activation=double_tanh)(lstm_1)\n",
    "    return keras.Model(inputs, outputs, name=f\"lstm_lstm_first_stage-lstm1_fc1{first_stage_lstm_hyper_param}\")\n",
    "\n",
    "\n",
    "inputs = Input(shape=(20, 1))\n",
    "lstm_1 = LSTM(units=20, kernel_regularizer=l1_l2(0.0, 0.0), bias_regularizer=l1_l2(0.0, 0.0))(inputs)\n",
    "outputs = Dense(units=21, activation=\"relu\")(lstm_1)\n",
    "first_stage_lstm_model = keras.Model(inputs, outputs, name=\"first_stage_lstm\")\n",
    "\n",
    "lstm_lstm_1st_stage_model = build_first_stage_lstm()\n",
    "lstm_lstm_1st_stage_model.summary()\n",
    "# lstm_lstm_1st_stage_model.compile(loss='mean_squared_error', optimizer='adam', metrics=first_stage_lstm_metrics )\n",
    "# train_history = lstm_lstm_1st_stage_model.fit(x=first_stage_lstm_train_X, y=first_stage_lstm_train_Y, validation_data=(first_stage_lstm_dev_X, first_stage_lstm_dev_Y), epochs=first_stage_lstm_max_epoch, batch_size=first_stage_lstm_batch_size, callbacks=first_stage_lstm_callbacks_list, shuffle=True, verbose=1)\n",
    "# best_epoch_num = np.argmin(np.array(train_history.history['val_loss'])) + 1\n",
    "# best_val_loss = train_history.history['val_loss'][best_epoch_num-1]\n",
    "# best_first_stage_lstm_weight_path = first_stage_lstm_model_dir/f\"epoch{best_epoch_num}_{best_val_loss:.5f}.h5\"\n",
    "# logging.info(f\"The best first stage lstm weight is: epoch{best_epoch_num}_{best_val_loss:.5f}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd3c44-28e6-43e9-8b83-a7d86f121938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def first_stage_lstm_model(lstm_weight_path: \"pathlib.PosixPath\", dataset: \"pd.DataFrame\", first_stage_lstm_result_path_basis: \"pathlib.PosixPath\", data_split_setting: str = \"\", save_file: bool = False) -> (\"pd.DataFrame\", \"pd.DataFrame\", \"pd.DataFrame\"):\n",
    "    best_first_stage_lstm_model = load_model(lstm_weight_path, custom_objects={'double_tanh':double_tanh})\n",
    "    pred_input_len = dataset.shape[1]-1\n",
    "    pred_input = dataset.iloc[::, :pred_input_len].values.reshape(-1, pred_input_len, 1)\n",
    "    dataset.columns = pd.RangeIndex(dataset.shape[1])  # in order to align dataset & first_stage_lstm_output_df\n",
    "    fisrt_stage_lstm_pred = best_first_stage_lstm_model.predict(pred_input)\n",
    "    first_stage_lstm_output_df = pd.DataFrame(fisrt_stage_lstm_pred, index=dataset.index)\n",
    "    first_stage_lstm_resid_df = dataset - first_stage_lstm_output_df\n",
    "\n",
    "    if save_file:\n",
    "        first_stage_lstm_output_df.to_csv(first_stage_lstm_result_path_basis.parent/(str(first_stage_lstm_result_path_basis.stem) + f'-first_stage_lstm_output{data_split_setting}.csv'))\n",
    "        first_stage_lstm_resid_df.to_csv(first_stage_lstm_result_path_basis.parent/(str(first_stage_lstm_result_path_basis.stem) + f'-first_stage_lstm_resid{data_split_setting}.csv'))\n",
    "\n",
    "    return first_stage_lstm_output_df, first_stage_lstm_resid_df\n",
    "\n",
    "first_stage_lstm_result_path_basis = first_stage_lstm_result_dir/f'{output_file_name}.csv'\n",
    "first_stage_lstm_result_paths = []\n",
    "first_stage_lstm_result_types = [\"-first_stage_lstm_output\", \"-first_stage_lstm_resid\"]\n",
    "\n",
    "for data_sp_setting in data_split_settings:\n",
    "    for first_stage_lstm_result_type in first_stage_lstm_result_types:\n",
    "        first_stage_lstm_result_paths.append(first_stage_lstm_result_dir/f'{output_file_name}{first_stage_lstm_result_type}{data_sp_setting}.csv')\n",
    "\n",
    "if all([df_path.exists() for df_path in first_stage_lstm_result_paths]):\n",
    "    pass\n",
    "else:\n",
    "    for (data_sp_setting, dataset) in tqdm(zip(data_split_settings, corr_datasets)):\n",
    "         first_stage_lstm_model(best_first_stage_lstm_weight_path, dataset, first_stage_lstm_result_path_basis=first_stage_lstm_result_path_basis, data_split_setting=data_sp_setting, save_file=save_lstm_resid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM for second stage prediction (for residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060591b9-a732-449d-bcc8-e183e34f9cdc",
   "metadata": {},
   "source": [
    "## settings of input data of second stage LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed36f1c-62c7-4874-a3a4-1aaf35bcdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.from_tensor_slices(dict(pd.read_csv(f'./dataset/after_arima/arima_resid_train.csv')))\n",
    "second_stage_lstm_train_X = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_train.csv', index_col=[\"items\"]).iloc[::, :-1]\n",
    "second_stage_lstm_train_Y = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_train.csv', index_col=[\"items\"]).iloc[::, -1]\n",
    "second_stage_lstm_dev_X = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_dev.csv', index_col=[\"items\"]).iloc[::, :-1]\n",
    "second_stage_lstm_dev_Y = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_dev.csv', index_col=[\"items\"]).iloc[::, -1]\n",
    "second_stage_lstm_test1_X = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_test1.csv', index_col=[\"items\"]).iloc[::, :-1]\n",
    "second_stage_lstm_test1_Y = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_test1.csv', index_col=[\"items\"]).iloc[::, -1]\n",
    "second_stage_lstm_test2_X = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_test2.csv', index_col=[\"items\"]).iloc[::, :-1]\n",
    "second_stage_lstm_test2_Y = pd.read_csv(first_stage_lstm_result_dir/f'{output_file_name}-first_stage_lstm_resid-data_sp_test2.csv', index_col=[\"items\"]).iloc[::, -1]\n",
    "\n",
    "second_stage_lstm_X_len = second_stage_lstm_train_X.shape[1]\n",
    "second_stage_lstm_Y_len = second_stage_lstm_train_Y.shape[1] if len(second_stage_lstm_train_Y.shape)>1 else 1\n",
    "second_stage_lstm_train_X = second_stage_lstm_train_X.values.reshape(-1, second_stage_lstm_X_len, 1)\n",
    "second_stage_lstm_train_Y = second_stage_lstm_train_Y.values.reshape(-1, second_stage_lstm_Y_len)\n",
    "second_stage_lstm_dev_X = second_stage_lstm_dev_X.values.reshape(-1, second_stage_lstm_X_len, 1)\n",
    "second_stage_lstm_dev_Y = second_stage_lstm_dev_Y.values.reshape(-1, second_stage_lstm_Y_len)\n",
    "second_stage_lstm_test1_X = second_stage_lstm_test1_X.values.reshape(-1, second_stage_lstm_X_len, 1)\n",
    "second_stage_lstm_test1_Y = second_stage_lstm_test1_Y.values.reshape(-1, second_stage_lstm_Y_len)\n",
    "second_stage_lstm_test2_X = second_stage_lstm_test2_X.values.reshape(-1, second_stage_lstm_X_len, 1)\n",
    "second_stage_lstm_test2_Y = second_stage_lstm_test2_Y.values.reshape(-1, second_stage_lstm_Y_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd276f-9102-4678-ba03-26036955e8eb",
   "metadata": {},
   "source": [
    "## settings of second stage LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4ad8b6-cbec-4ab1-980a-942ddc0da28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.26 ms (started: 2022-12-12 14:47:23 +00:00)\n"
     ]
    }
   ],
   "source": [
    "second_stage_lstm_model_log = TensorBoard(log_dir=second_stage_lstm_log_dir)\n",
    "second_stage_lstm_max_epoch = 5000\n",
    "second_stage_lstm_batch_size = 64\n",
    "second_stage_lstm_metrics = ['mse', 'mae']\n",
    "second_stage_lstm_opt_adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "if second_stage_lstm_hyper_param == \"-kS_hyper\":\n",
    "    lstm_layer = LSTM(units=10, kernel_regularizer=l1_l2(0.2, 0.0), bias_regularizer=l1_l2(0.2, 0.0), activation=\"tanh\", dropout=0.1, name=f\"lstm{second_stage_lstm_hyper_param}\")  # LSTM hyper params from 【Something Old, Something New — A Hybrid Approach with ARIMA and LSTM to Increase Portfolio Stability】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df49f64-a93a-4104-b07d-dfbd0a688004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_lstm_second_stage-lstm1_fc1-kS_hyper\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " lstm-kS_hyper (LSTM)        (None, 10)                480       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 157 ms (started: 2022-12-12 14:47:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) *2)\n",
    "\n",
    "\n",
    "def build_second_stage_lstm():\n",
    "    inputs = Input(shape=(20, 1))\n",
    "    lstm_1 = lstm_layer(inputs)\n",
    "    outputs = Dense(units=1, activation=double_tanh)(lstm_1)\n",
    "    return keras.Model(inputs, outputs, name=f\"lstm_lstm_second_stage-lstm1_fc1{second_stage_lstm_hyper_param}\")\n",
    "\n",
    "lstm_model = build_second_stage_lstm()\n",
    "lstm_model.summary()\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=second_stage_lstm_opt_adam , metrics=second_stage_lstm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2b47f-ad31-431f-8ded-1e2acf98d7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_csv_path = res_dir/f'{output_file_name}{second_stage_lstm_hyper_param}-second_stage_lstm_evaluation.csv'\n",
    "res_csv_path.touch(exist_ok=True)\n",
    "with open(res_csv_path, 'r+') as f:\n",
    "    if not f.read():\n",
    "        f.write(\"epoch,TRAIN_MSE,DEV_MSE,TEST1_MSE,TEST2_MSE,TRAIN_MAE,DEV_MAE,TEST1_MAE,TEST2_MAE\")\n",
    "\n",
    "res_df = pd.read_csv(res_csv_path)\n",
    "saved_model_list = [int(p.stem[p.stem.find(\"epoch\")+len(\"epoch\"):]) for p in  second_stage_lstm_model_dir.glob('*.h5')]\n",
    "epoch_start = max(saved_model_list) if saved_model_list else 1\n",
    "\n",
    "try:\n",
    "    for epoch_num in tqdm(range(epoch_start, second_stage_lstm_max_epoch)):\n",
    "        if epoch_num > 1:\n",
    "            lstm_model = load_model(second_stage_lstm_model_dir/f\"{output_file_name}{second_stage_lstm_hyper_param}-epoch{epoch_num - 1}.h5\", custom_objects={'double_tanh':double_tanh})\n",
    "\n",
    "        save_model = ModelCheckpoint(second_stage_lstm_model_dir/f\"{output_file_name}{second_stage_lstm_hyper_param}-epoch{epoch_num}.h5\",\n",
    "                                                     monitor='loss', verbose=1, mode='min', save_best_only=False)\n",
    "        lstm_model.fit(second_stage_lstm_train_X, second_stage_lstm_train_Y, epochs=1, batch_size=second_stage_lstm_batch_size, callbacks=[second_stage_lstm_model_log, save_model], shuffle=True, verbose=0)\n",
    "\n",
    "        # test the model\n",
    "        score_train = lstm_model.evaluate(second_stage_lstm_train_X, second_stage_lstm_train_Y)\n",
    "        score_dev = lstm_model.evaluate(second_stage_lstm_dev_X, second_stage_lstm_dev_Y)\n",
    "        score_test1 = lstm_model.evaluate(second_stage_lstm_test1_X, second_stage_lstm_test1_Y)\n",
    "        score_test2 = lstm_model.evaluate(second_stage_lstm_test2_X, second_stage_lstm_test2_Y)\n",
    "        metrics_mse_ind = second_stage_lstm_metrics.index('mse') + 1  # need to plus one, because first term of lstm_model.evaluate() is loss\n",
    "        metrics_mae_ind = second_stage_lstm_metrics.index('mae') + 1  # need to plus one, because first term of lstm_model.evaluate() is loss\n",
    "        res_each_epoch_df = pd.DataFrame(np.array([epoch_num, score_train[metrics_mse_ind], score_dev[metrics_mse_ind],\n",
    "                                                   score_test1[metrics_mse_ind], score_test2[metrics_mse_ind],\n",
    "                                                   score_train[metrics_mae_ind], score_dev[metrics_mae_ind],\n",
    "                                                   score_test1[metrics_mae_ind], score_test2[metrics_mae_ind]]).reshape(-1, 9),\n",
    "                                        columns=[\"epoch\", \"TRAIN_MSE\", \"DEV_MSE\", \"TEST1_MSE\", \n",
    "                                                 \"TEST2_MSE\", \"TRAIN_MAE\", \"DEV_MAE\",\n",
    "                                                 \"TEST1_MAE\",\"TEST2_MAE\"])\n",
    "        res_df = pd.concat([res_df, res_each_epoch_df])\n",
    "        if (res_df.shape[0] % 100) == 0:\n",
    "            res_df.to_csv(res_csv_path, index=False)  # insurance for 『finally』 part doesent'work\n",
    "except Exception as e:\n",
    "    error_class = e.__class__.__name__  # 取得錯誤類型\n",
    "    detail = e.args[0]  # 取得詳細內容\n",
    "    cl, exc, tb = sys.exc_info()  # 取得Call Stack\n",
    "    last_call_stack = traceback.extract_tb(tb)[-1]  # 取得Call Stack的最後一筆資料\n",
    "    file_name = last_call_stack[0]  # 取得發生的檔案名稱\n",
    "    line_num = last_call_stack[1]  # 取得發生的行號\n",
    "    func_name = last_call_stack[2]  # 取得發生的函數名稱\n",
    "    err_msg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(file_name, line_num, func_name, error_class, detail)\n",
    "    logging.error(err_msg)\n",
    "else:\n",
    "    pass\n",
    "finally:\n",
    "    res_df.to_csv(res_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094d1873-1173-496b-87ea-50cd9031e8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
