{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b7d45-6914-4fab-89f4-9e65fa9dd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "err_log_handler = logging.FileHandler(filename=\"./models/arima_train_err_log.txt\", mode='a')\n",
    "err_logger = logging.getLogger(\"arima_train_err\")\n",
    "err_logger.addHandler(err_log_handler)\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeff40e-d0b9-46ba-ab97-9b782de39af1",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3faba9-e37c-4c4a-93e4-e3bb0c4597ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting of output files\n",
    "save_raw_corr_data = True\n",
    "save_train_info_arima_resid_data = True\n",
    "# data implement setting\n",
    "data_implement = \"sp500_20082017\"  # tw50|sp500_20082017|sp500_19972007|tetuan_power\n",
    "                                                          # |sp500_20082017_consumer_discretionary\n",
    "# train set setting\n",
    "items_setting = \"train\"  # train|all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe208abf-0ea9-49b2-b8ba-334d745fde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading & implement setting\n",
    "dataset_path = Path(\"../dataset/\")\n",
    "if data_implement == \"tw50\":\n",
    "    file_name = Path(\"tw50_hold_20082018_adj_close_pre.csv\")\n",
    "    train_set = ['萬海_adj_close', '豐泰_adj_close', '友達_adj_close', '欣興_adj_close', '台塑化_adj_close', '和泰車_adj_close', '元大金_adj_close', '南電_adj_close', '台塑_adj_close', '統一超_adj_close', '台泥_adj_close', '瑞昱_adj_close', '彰銀_adj_close', '富邦金_adj_close', '研華_adj_close', '中鋼_adj_close', '鴻海_adj_close', '台新金_adj_close', '遠傳_adj_close', '南亞_adj_close', '台達電_adj_close', '台灣大_adj_close', '台化_adj_close', '聯詠_adj_close', '廣達_adj_close', '聯發科_adj_close', '台積電_adj_close', '統一_adj_close', '中信金_adj_close', '長榮_adj_close']\n",
    "elif data_implement == \"sp500_19972007\":\n",
    "    file_name = Path(\"sp500_hold_19972007_adj_close_pre.csv\")\n",
    "    train_set = ['PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'NEM', 'CTAS', 'MAT', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'CI', 'ZION', 'COO', 'FDX', 'GLW', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'BMY', 'KMB', 'JPM', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'WMB', 'IFF', 'CMS', 'MMC', 'REG', 'ES', 'ITW', 'VRTX', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'VNO', 'WDC', 'PVH', 'NOC', 'PCAR', 'NSC', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'ALK', 'TAP', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'HIG', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'CMA', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG']\n",
    "elif data_implement in [\"sp500_20082017\", \"paper_eva_1\", \"paper_eva_2\", \"paper_eva_3\", \"paper_eva_4\", \"paper_eva_5\"]:\n",
    "    file_name = Path(\"sp500_hold_20082017_adj_close_pre.csv\")\n",
    "    train_set = ['CELG', 'PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'CRM', 'NEM', 'JNPR', 'LB', 'CTAS', 'MAT', 'MDLZ', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'GRMN', 'CI', 'ZION', 'COO', 'TIF', 'RHT', 'FDX', 'LLL', 'GLW', 'GPN', 'IPGP', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'AAP', 'DAL', 'A', 'MON', 'BRK', 'BMY', 'KMB', 'JPM', 'CCI', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'UPS', 'WMB', 'IFF', 'CMS', 'ARNC', 'VIAB', 'MMC', 'REG', 'ES', 'ITW', 'NDAQ', 'AIZ', 'VRTX', 'CTL', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'EXR', 'VNO', 'BBT', 'WDC', 'UAL', 'PVH', 'NOC', 'PCAR', 'NSC', 'UAA', 'FFIV', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'CMG', 'ALK', 'ULTA', 'TMK', 'TAP', 'SCG', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'WU', 'ACN', 'HIG', 'TEL', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'ETFC', 'CMA', 'NRG', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'CBS', 'ALGN', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'XLNX', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG', 'FOX', 'MA']\n",
    "elif data_implement == \"tetuan_power\":\n",
    "    file_name = Path(\"Tetuan City power consumption_pre.csv\")\n",
    "    train_set = [\"Temperature\", \"Humidity\", \"Wind Speed\", \"general diffuse flows\", \"diffuse flows\", \"Zone 1 Power Consumption\", \"Zone 2 Power Consumption\", \"Zone 3 Power Consumption\"]\n",
    "elif data_implement == \"sp500_20082017_consumer_discretionary\":\n",
    "    file_name = Path(\"sp500_hold_20082017_adj_close_pre_consumer_discretionary.csv\")\n",
    "    train_set = ['LKQ', 'LEN', 'TGT', 'YUM', 'TJX', 'GRMN', 'MCD', 'DRI', 'HBI', 'GPS', 'SBUX', 'TSCO', 'WYN', 'MGM', 'MAT', 'ROST', 'IPG', 'PVH', 'VFC', 'EXPE', 'JWN', 'GPC', 'DIS', 'FL', 'AAP', 'KSS', 'TIF', 'HAS', 'DHI', 'MHK', 'UAA', 'KMX', 'BBY', 'CMCSA', 'LEG', 'VIAB', 'CCL', 'LB', 'HOG', 'F', 'AZO', 'RL', 'DISCA', 'FOXA', 'PHM', 'AMZN', 'WHR', 'NKE', 'SNA', 'M', 'FOX', 'ULTA', 'GT', 'CMG', 'LOW', 'TWX', 'HD', 'CBS']\n",
    "\n",
    "\n",
    "dataset_df = pd.read_csv(dataset_path/file_name)\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns.values[1:])  # all data\n",
    "test_set = [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# train set setting\n",
    "if items_setting == \"all\":\n",
    "    items_set = all_set\n",
    "    output_set_name = \"_all\"\n",
    "elif items_setting == \"train\":\n",
    "    items_set = train_set\n",
    "    output_set_name = \"_train\"\n",
    "train_info = {\"tw50\": {\"items\":items_set, \"output_file_name_basis\": \"tw50_20082017\"},\n",
    "              \"sp500_19972007\": {\"items\":items_set, \"output_file_name_basis\": f\"sp500_19972007\"},\n",
    "              \"sp500_20082017\": {\"items\": items_set, \"output_file_name_basis\": f\"sp500_20082017\"},\n",
    "              \"tetuan_power\": {\"items\": items_set, \"output_file_name_basis\":  f\"tetuan_power\"},\n",
    "              \"sp500_20082017_consumer_discretionary\": {\"items\": items_set, \"output_file_name_basis\":  f\"sp500_20082017_consumer_discretionary\"}}\n",
    "items_implement = train_info[data_implement]['items']\n",
    "logging.info(f\"===== len(train set): {len(items_implement)} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = train_info[data_implement]['output_file_name_basis'] + output_set_name\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")\n",
    "\n",
    "# display(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa09458-77c5-4dbe-90a7-5e99929f9bab",
   "metadata": {},
   "source": [
    "## Load or Create Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20cb71-8a6d-4bf8-8808-f03f8577d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_window = 100\n",
    "corr_stride = 100\n",
    "data_length = int(len(dataset_df)/corr_window)*corr_window\n",
    "corr_ind = list(range(99, 2400, corr_stride))  + list(range(99+20, 2500, corr_stride)) + \\\n",
    "           list(range(99+40, 2500, corr_stride)) + list(range(99+60, 2500, corr_stride)) + \\\n",
    "           list(range(99+80, 2500, corr_stride))  # only suit for settings of paper\n",
    "\n",
    "corr_series_length = int((data_length-corr_window)/corr_stride)\n",
    "corr_series_length_paper = 21  # only suit for settings of paper\n",
    "data_diverse_stride = 20  # only suit for settings of paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739c702-6558-45cc-b2ba-3fa3db2e1107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_data_corr(items: list, corr_ind: list) -> \"pd.DataFrame\":\n",
    "    tmp_corr = dataset_df[items[0]].rolling(window=corr_window).corr(dataset_df[items[1]])\n",
    "    tmp_corr = tmp_corr.iloc[corr_ind]\n",
    "    data_df = pd.DataFrame(tmp_corr.values.reshape(-1, corr_series_length), columns=tmp_corr.index[:corr_series_length], dtype=\"float32\")\n",
    "    ind = [f\"{items[0]} & {items[1]}_{i}\" for i in range(0, 100, data_diverse_stride)]\n",
    "    data_df.index = ind\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def gen_train_data(items: list, corr_ind: list, save_file: bool = False)-> \"four pd.DataFrame\":\n",
    "    train_df = pd.DataFrame(dtype=\"float32\")\n",
    "    dev_df = pd.DataFrame(dtype=\"float32\")\n",
    "    test1_df = pd.DataFrame(dtype=\"float32\")\n",
    "    test2_df = pd.DataFrame(dtype=\"float32\")\n",
    "\n",
    "    for pair in tqdm(combinations(items, 2)):\n",
    "        data_df = gen_data_corr([pair[0], pair[1]], corr_ind=corr_ind)\n",
    "        train_df = pd.concat([train_df, data_df.iloc[:, 0:21]])\n",
    "        dev_df = pd.concat([dev_df, data_df.iloc[:, 1:22]])\n",
    "        test1_df = pd.concat([test1_df, data_df.iloc[:, 2:23]])\n",
    "        test2_df = pd.concat([test2_df, data_df.iloc[:, 3:24]])\n",
    "\n",
    "    if save_file:\n",
    "        before_arima_data_path = dataset_path/f\"{output_file_name}_before_arima\"\n",
    "        before_arima_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        train_df.to_csv(before_arima_data_path/f\"{output_file_name}_train.csv\")\n",
    "        dev_df.to_csv(before_arima_data_path/f\"{output_file_name}_dev.csv\")\n",
    "        test1_df.to_csv(before_arima_data_path/f\"{output_file_name}_test1.csv\")\n",
    "        test2_df.to_csv(before_arima_data_path/f\"{output_file_name}_test2.csv\")\n",
    "\n",
    "    return train_df, dev_df, test1_df, test2_df\n",
    "\n",
    "\n",
    "before_arima_data_path = dataset_path/f\"{output_file_name}_before_arima\"\n",
    "train_df_file = before_arima_data_path/f\"{output_file_name}_train.csv\"\n",
    "dev_df_file = before_arima_data_path/f\"{output_file_name}_dev.csv\"\n",
    "test1_df_file = before_arima_data_path/f\"{output_file_name}_test1.csv\"\n",
    "test2_df_file = before_arima_data_path/f\"{output_file_name}_test2.csv\"\n",
    "all_df_file = [train_df_file, dev_df_file, test1_df_file, test2_df_file]\n",
    "if any([df_file.exists() for df_file in all_df_file]):\n",
    "    corr_datasets = [pd.read_csv(df_file).set_index(\"Unnamed: 0\") for df_file in all_df_file]\n",
    "else:\n",
    "    corr_datasets = gen_train_data(items_implement, corr_ind, save_file = save_raw_corr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91fb23-de60-498b-90cb-d59db9886ece",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193f34d-878e-4a5a-9da7-8a024e887c8d",
   "metadata": {},
   "source": [
    "## settings of input data of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689197d-0ec3-45ba-937d-b9c3d3050d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Dev - Test Generation\n",
    "train_X = corr_datasets[0].iloc[:, :-1]\n",
    "dev_X = corr_datasets[1].iloc[:, :-1]\n",
    "test1_X = corr_datasets[2].iloc[:, :-1]\n",
    "test2_X = corr_datasets[3].iloc[:, :-1]\n",
    "train_Y = corr_datasets[0].iloc[:, -1]\n",
    "dev_Y = corr_datasets[1].iloc[:, -1]\n",
    "test1_Y = corr_datasets[2].iloc[:, -1]\n",
    "test2_Y = corr_datasets[3].iloc[:, -1]\n",
    "\n",
    "\n",
    "# data sampling\n",
    "STEP = 20\n",
    "\n",
    "lstm_train_X = train_X.values.reshape(-1, 20, 1)\n",
    "lstm_train_Y = train_Y.values.reshape(-1, 1)\n",
    "lstm_dev_X = dev_X.values.reshape(-1, 20, 1)\n",
    "lstm_dev_Y = dev_Y.values.reshape(-1, 1)\n",
    "lstm_test1_X = test1_X.values.reshape(-1, 20, 1)\n",
    "lstm_test1_Y = test1_Y.values.reshape(-1, 1)\n",
    "lstm_test2_X = test2_X.values.reshape(-1, 20, 1)\n",
    "lstm_test2_Y = test2_Y.values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# _train_X = np.asarray(train_X).reshape((int(1117500/STEP), 20, 1))\n",
    "# _dev_X = np.asarray(dev_X).reshape((int(1117500/STEP), 20, 1))\n",
    "# _test1_X = np.asarray(test1_X).reshape((int(1117500/STEP), 20, 1))\n",
    "# _test2_X = np.asarray(test2_X).reshape((int(1117500/STEP), 20, 1))\n",
    "\n",
    "# _train_Y = np.asarray(train_Y).reshape(int(1117500/STEP), 1)\n",
    "# _dev_Y = np.asarray(dev_Y).reshape(int(1117500/STEP), 1)\n",
    "# _test1_Y = np.asarray(test1_Y).reshape(int(1117500/STEP), 1)\n",
    "# _test2_Y = np.asarray(test2_Y).reshape(int(1117500/STEP), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76572ccd-5dfd-4d0f-8d80-fecae7ae6277",
   "metadata": {},
   "source": [
    "## settings of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc892b-8725-454d-8ece-823ad4995a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_layer = LSTM(units=10, kernel_regularizer=l1_l2(0.2, 0.0), bias_regularizer=l1_l2(0.2, 0.0), activation=\"tanh\", dropout=0.1)  # LSTM hyper params from 【Something Old, Something New — A Hybrid Approach with ARIMA and LSTM to Increase Portfolio Stability】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb8d60-b046-431d-943c-dda32147505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) *2)\n",
    "\n",
    "\n",
    "def build_many_one_lstm():\n",
    "    inputs = Input(shape=(20, 1))\n",
    "    lstm_1 = lstm_layer(inputs)\n",
    "    outputs = Dense(units=1, activation=double_tanh)(lstm_1)\n",
    "    return keras.Model(inputs, outputs, name=\"many_one_lstm\")\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "lstm_model = build_many_one_lstm()\n",
    "lstm_model.summary()\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=opt, metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864ac95-9b14-4bce-ae1f-42696e5dda6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = Path('./models/')\n",
    "log_dir = Path('./models/lstm_train_logs/')\n",
    "res_dir = Path('./results/')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_csv_path = res_dir/f'{output_file_name}_LSTM_only_evaluation.csv'\n",
    "res_csv_path.touch(exist_ok=True)\n",
    "with open(res_csv_path, 'r+') as f:\n",
    "    if not f.read():\n",
    "        f.write(\"epoch,TRAIN_MSE,DEV_MSE,TEST1_MSE,TEST2_MSE,TRAIN_MAE,DEV_MAE,TEST1_MAE,TEST2_MAE\")\n",
    "\n",
    "res_df = pd.read_csv(res_csv_path)\n",
    "saved_model_list = [int(p.stem.split('_')[1]) for p in model_dir.glob('*.h5')]\n",
    "model_cbk = TensorBoard(log_dir=log_dir)\n",
    "epoch_start = max(saved_model_list) if saved_model_list else 1\n",
    "max_epoch = 5000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch_num in tqdm(range(epoch_start, max_epoch)):\n",
    "    if epoch_num > 1:\n",
    "        lstm_model = load_model(model_dir/f\"{output_file_name}_LSTM_only_epoch_{epoch_num - 1}.h5\", custom_objects={'double_tanh':double_tanh})\n",
    "\n",
    "    save_model = ModelCheckpoint(model_dir/f\"{output_file_name}_LSTM_only_epoch_{epoch_num}.h5\",\n",
    "                                                 monitor='loss', verbose=1, mode='min', save_best_only=False)\n",
    "    lstm_model.fit(lstm_train_X, lstm_train_Y, epochs=1, batch_size=batch_size, shuffle=True, callbacks=[model_cbk, save_model])\n",
    "\n",
    "    # test the model\n",
    "    score_train = lstm_model.evaluate(lstm_train_X, lstm_train_Y)\n",
    "    score_dev = lstm_model.evaluate(lstm_dev_X, lstm_dev_Y)\n",
    "    score_test1 = lstm_model.evaluate(lstm_test1_X, lstm_test1_Y)\n",
    "    score_test2 = lstm_model.evaluate(lstm_test2_X, lstm_test2_Y)\n",
    "    res_each_epoch_df = pd.DataFrame(np.array([epoch_num, score_train[0], score_dev[0], \n",
    "                                               score_test1[0], score_test2[0], \n",
    "                                               score_train[1], score_dev[1], \n",
    "                                               score_test1[1], score_test2[1]]).reshape(-1, 9),\n",
    "                                    columns=[\"epoch\", \"TRAIN_MSE\", \"DEV_MSE\", \"TEST1_MSE\", \n",
    "                                             \"TEST2_MSE\", \"TRAIN_MAE\", \"DEV_MAE\",\n",
    "                                             \"TEST1_MAE\",\"TEST2_MAE\"])\n",
    "    res_df = pd.concat([res_df, res_each_epoch_df])\n",
    "\n",
    "res_df.to_csv(res_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab821d8d-8257-41c3-b22f-ec257da5f053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def double_tanh(x):\n",
    "#     return (K.tanh(x) * 2)\n",
    "\n",
    "# #get_custom_objects().update({'double_tanh':Activation(double_tanh)})\n",
    "\n",
    "# # Model Generation\n",
    "# model = Sequential()\n",
    "# #check https://machinelearningmastery.com/use-weight-regularization-lstm-networks-time-series-forecasting/\n",
    "# model.add(LSTM(25, input_shape=(20,1), dropout=0.0, kernel_regularizer=l1_l2(0.00,0.00), bias_regularizer=l1_l2(0.00,0.00)))\n",
    "# model.add(Dense(1, activation=double_tanh))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])\n",
    "# #, kernel_regularizer=l1_l2(0,0.1), bias_regularizer=l1_l2(0,0.1),\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39eef9a-351b-459a-bdc5-f497d1bd450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fitting the Model\n",
    "# model_scores = {}\n",
    "# Reg = False\n",
    "# d = 'LSTM_only_new_api'\n",
    "\n",
    "# if Reg :\n",
    "#     d += '_with_reg'\n",
    "\n",
    "# epoch_num=1\n",
    "# max_epoch = 3500\n",
    "# for _ in range(max_epoch):\n",
    "\n",
    "#     # train the model\n",
    "#     dir_ = './lstm_only_models/'+d\n",
    "#     file_list = os.listdir(dir_)\n",
    "#     if len(file_list) != 0 :\n",
    "#         epoch_num = len(file_list) + 1\n",
    "#         recent_model_name = 'epoch'+str(epoch_num-1)\n",
    "#         filepath = './lstm_only_models/' + d + '/' + recent_model_name\n",
    "#         # custom_objects = {\"double_tanh\": double_tanh}\n",
    "#         # with keras.utils.custom_object_scope(custom_objects):\n",
    "#         model = load_model(filepath)\n",
    "\n",
    "#     filepath = './lstm_only_models/' + d + '/epoch'+str(epoch_num)\n",
    "\n",
    "#     # checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=False, mode='min')\n",
    "#     model.fit(_train_X, _train_Y, epochs=1, batch_size=500, shuffle=True)\n",
    "#     model.save(filepath)\n",
    "    \n",
    "#     #callbacks_list = [checkpoint]\n",
    "#     #if len(callbacks_list) == 0:\n",
    "#     #    model.fit(_train_X, _train_Y, epochs=1, batch_size=500, shuffle=True)\n",
    "#     #else:\n",
    "#     #    model.fit(_train_X, _train_Y, epochs=1, batch_size=500, shuffle=True, callbacks=callbacks_list)\n",
    "\n",
    "#     # test the model\n",
    "#     score_train = model.evaluate(_train_X, _train_Y)\n",
    "#     score_dev = model.evaluate(_dev_X, _dev_Y)\n",
    "#     score_test1 = model.evaluate(_test1_X, _test1_Y)\n",
    "#     score_test2 = model.evaluate(_test2_X, _test2_Y)\n",
    "\n",
    "#     print('train set score : mse - ' + str(score_train[1]) +' / mae - ' + str(score_train[2]))\n",
    "#     print('dev set score : mse - ' + str(score_dev[1]) +' / mae - ' + str(score_dev[2]))\n",
    "#     print('test1 set score : mse - ' + str(score_test1[1]) +' / mae - ' + str(score_test1[2]))\n",
    "#     print('test2 set score : mse - ' + str(score_test2[1]) +' / mae - ' + str(score_test2[2]))\n",
    "# #.history['mean_squared_error'][0]\n",
    "#     # get former score data\n",
    "#     df = pd.read_csv(\"./lstm_only_scores/\"+d+\".csv\")\n",
    "#     train_mse = list(df['TRAIN_MSE'])\n",
    "#     dev_mse = list(df['DEV_MSE'])\n",
    "#     test1_mse = list(df['TEST1_MSE'])\n",
    "#     test2_mse = list(df['TEST2_MSE'])\n",
    "\n",
    "#     train_mae = list(df['TRAIN_MAE'])\n",
    "#     dev_mae = list(df['DEV_MAE'])\n",
    "#     test1_mae = list(df['TEST1_MAE'])\n",
    "#     test2_mae = list(df['TEST2_MAE'])\n",
    "\n",
    "#     # append new data\n",
    "#     train_mse.append(score_train[1])\n",
    "#     dev_mse.append(score_dev[1])\n",
    "#     test1_mse.append(score_test1[1])\n",
    "#     test2_mse.append(score_test2[1])\n",
    "\n",
    "#     train_mae.append(score_train[2])\n",
    "#     dev_mae.append(score_dev[2])\n",
    "#     test1_mae.append(score_test1[2])\n",
    "#     test2_mae.append(score_test2[2])\n",
    "\n",
    "#     # organize newly created score dataset\n",
    "#     model_scores['TRAIN_MSE'] = train_mse\n",
    "#     model_scores['DEV_MSE'] = dev_mse\n",
    "#     model_scores['TEST1_MSE'] = test1_mse\n",
    "#     model_scores['TEST2_MSE'] = test2_mse\n",
    "\n",
    "#     model_scores['TRAIN_MAE'] = train_mae\n",
    "#     model_scores['DEV_MAE'] = dev_mae\n",
    "#     model_scores['TEST1_MAE'] = test1_mae\n",
    "#     model_scores['TEST2_MAE'] = test2_mae\n",
    "    \n",
    "#     # save newly created score dataset\n",
    "#     model_scores_df = pd.DataFrame(model_scores)\n",
    "#     model_scores_df.to_csv(\"./lstm_only_scores/\"+d+\".csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
