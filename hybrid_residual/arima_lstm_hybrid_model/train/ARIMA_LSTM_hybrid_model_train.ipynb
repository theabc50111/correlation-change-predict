{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41660f84-97c9-4162-a708-f601b3b7a3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.01 s (started: 2022-12-05 02:57:11 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pprint import pformat\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"/tf/correlation-coef-predict/ywt_library\")\n",
    "import data_generation\n",
    "from data_generation import data_gen_cfg\n",
    "from ywt_arima import arima_model, arima_err_logger_init\n",
    "\n",
    "with open('../../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.debug(pformat(data_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcae291-e762-4e5d-8740-05e845af7a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data implement & output setting & trainset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167474f5-dd41-487c-a140-54d699e47621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 558 Âµs (started: 2022-12-05 02:57:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# setting of output files\n",
    "save_corr_data = True\n",
    "save_arima_resid_data = True\n",
    "# data implement setting\n",
    "data_implement = \"SP500_20082017_CORR_SER_REG_CORR_MAT_HRCHY_11_CLUSTER\"  # watch options by operate: print(data_cfg[\"DATASETS\"].keys())\n",
    "# train set setting\n",
    "train_items_setting = \"-train_train\"  # -train_train|-train_all\n",
    "# data split  period setting, only suit for only settings of Korean paper\n",
    "data_split_settings = [\"-data_sp_train\", \"-data_sp_dev\", \"-data_sp_test1\", \"-data_sp_test2\", ]\n",
    "# lstm_hyper_params\n",
    "lstm_hyper_param = \"-kS_hyper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4fc57b1-5e84-49a3-8e15-bb6ae816af9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:===== len(train_set): 66, len(all_set): 97, len(test_set): 31 =====\n",
      "INFO:root:===== len(train set): 66 =====\n",
      "INFO:root:===== file_name basis:sp500_20082017_corr_ser_reg_corr_mat_hrchy_11_cluster-train_train =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.9 ms (started: 2022-12-05 02:57:13 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# data loading & implement setting\n",
    "dataset_df = pd.read_csv(data_cfg[\"DATASETS\"][data_implement]['FILE_PATH'])\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns)  # all data\n",
    "train_set = data_cfg[\"DATASETS\"][data_implement]['TRAIN_SET']\n",
    "test_set = data_cfg['DATASETS'][data_implement]['TEST_SET'] if data_cfg['DATASETS'][data_implement].get('TEST_SET') else [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# train items implement settings\n",
    "items_implement = train_set if train_items_setting == \"-train_train\" else all_set\n",
    "logging.info(f\"===== len(train set): {len(items_implement)} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + train_items_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")\n",
    "# display(dataset_df)\n",
    "\n",
    "# output folder settings\n",
    "corr_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-corr_data\"\n",
    "arima_result_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-arima_res\"\n",
    "model_dir = Path('./save_models/')\n",
    "lstm_log_dir = Path('./save_models/lstm_train_logs/')\n",
    "res_dir = Path('./results/')\n",
    "corr_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "arima_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "lstm_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe3cab1-9974-4aac-a85c-f418ab7395c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load or Create Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8090b6a-e720-413f-9e59-1f512ff0dc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_length = int(len(dataset_df)/data_gen_cfg[\"CORR_WINDOW\"])*data_gen_cfg[\"CORR_WINDOW\"]\n",
    "corr_ser_len_max = int((data_length-data_gen_cfg[\"CORR_WINDOW\"])/data_gen_cfg[\"CORR_STRIDE\"])\n",
    "\n",
    "train_df_path = corr_data_dir/f\"{output_file_name}-corr_train.csv\"\n",
    "dev_df_path = corr_data_dir/f\"{output_file_name}-corr_dev.csv\"\n",
    "test1_df_path = corr_data_dir/f\"{output_file_name}-corr_test1.csv\"\n",
    "test2_df_path = corr_data_dir/f\"{output_file_name}-corr_test2.csv\"\n",
    "all_corr_df_paths = dict(zip([\"train_df\", \"dev_df\", \"test1_df\", \"test2_df\"],\n",
    "                             [train_df_path, dev_df_path, test1_df_path, test2_df_path]))\n",
    "if all([df_path.exists() for df_path in all_corr_df_paths.values()]):\n",
    "    corr_datasets = [pd.read_csv(df_path).set_index(\"items\") for df_path in all_corr_df_paths.values()]\n",
    "else:\n",
    "    corr_datasets = data_generation.gen_train_data(items_implement, raw_data_df=dataset_df, corr_df_paths=all_corr_df_paths, corr_ser_len_max=corr_ser_len_max, save_file=save_corr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca1214-5327-4b3f-92b6-c9830bc5da50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arima_result_path_basis = arima_result_dir/f'{output_file_name}.csv'\n",
    "arima_result_paths = []\n",
    "arima_result_types = [\"-arima_model_info\", \"-arima_output\", \"-arima_resid\"]\n",
    "arima_err_logger_init(Path(os.path.abspath(''))/f\"save_models\")\n",
    "\n",
    "\n",
    "for data_sp_setting in data_split_settings:\n",
    "    for arima_result_type in arima_result_types:\n",
    "        arima_result_paths.append(arima_result_dir/f'{output_file_name}{arima_result_type}{data_sp_setting}.csv')\n",
    "\n",
    "if all([df_path.exists() for df_path in arima_result_paths]):\n",
    "    pass\n",
    "else:\n",
    "    for (data_sp_setting, dataset) in tqdm(zip(data_split_settings, corr_datasets)):\n",
    "        arima_model(dataset, arima_result_path_basis=arima_result_path_basis, data_split_setting=data_sp_setting, save_file=save_arima_resid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83a42a-7794-4cc4-9e1e-a924d716eec5",
   "metadata": {},
   "source": [
    "## settings of input data of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed36f1c-62c7-4874-a3a4-1aaf35bcdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.from_tensor_slices(dict(pd.read_csv(f'./dataset/after_arima/arima_resid_train.csv')))\n",
    "lstm_train_X = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_train.csv', index_col=\"items\").iloc[::, :-1]\n",
    "lstm_train_Y = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_train.csv', index_col=\"items\").iloc[::, -1]\n",
    "lstm_dev_X = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_dev.csv', index_col=\"items\").iloc[::, :-1]\n",
    "lstm_dev_Y = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_dev.csv', index_col=\"items\").iloc[::, -1]\n",
    "lstm_test1_X = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_test1.csv', index_col=\"items\").iloc[::, :-1]\n",
    "lstm_test1_Y = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_test1.csv', index_col=\"items\").iloc[::, -1]\n",
    "lstm_test2_X = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_test2.csv', index_col=\"items\").iloc[::, :-1]\n",
    "lstm_test2_Y = pd.read_csv(arima_result_dir/f'{output_file_name}-arima_resid-data_sp_test2.csv', index_col=\"items\").iloc[::, -1]\n",
    "\n",
    "lstm_X_len = lstm_train_X.shape[1]\n",
    "lstm_Y_len = lstm_train_Y.shape[1] if len(lstm_train_Y.shape)>1 else 1\n",
    "lstm_train_X = lstm_train_X.values.reshape(-1, lstm_X_len, 1)\n",
    "lstm_train_Y = lstm_train_Y.values.reshape(-1, lstm_Y_len)\n",
    "lstm_dev_X = lstm_dev_X.values.reshape(-1, lstm_X_len, 1)\n",
    "lstm_dev_Y = lstm_dev_Y.values.reshape(-1, lstm_Y_len)\n",
    "lstm_test1_X = lstm_test1_X.values.reshape(-1,  lstm_X_len, 1)\n",
    "lstm_test1_Y = lstm_test1_Y.values.reshape(-1, lstm_Y_len)\n",
    "lstm_test2_X = lstm_test2_X.values.reshape(-1,  lstm_X_len, 1)\n",
    "lstm_test2_Y = lstm_test2_Y.values.reshape(-1, lstm_Y_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c089da49-7fd9-4257-ac0e-c77418ec067a",
   "metadata": {},
   "source": [
    "## settings of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34446a-65bc-4b4d-9de4-186d285dcd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_log = TensorBoard(log_dir=lstm_log_dir)\n",
    "max_epoch = 5000\n",
    "batch_size = 64\n",
    "lstm_metrics = ['mse', 'mae']\n",
    "\n",
    "if lstm_hyper_param == \"-kS_hyper\":\n",
    "    lstm_layer = LSTM(units=10, kernel_regularizer=l1_l2(0.2, 0.0), bias_regularizer=l1_l2(0.2, 0.0), activation=\"tanh\", dropout=0.1, name=f\"lstm{lstm_hyper_param}\")  # LSTM hyper params from ãSomething Old, Something New â A Hybrid Approach with ARIMA and LSTM to Increase Portfolio Stabilityã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df49f64-a93a-4104-b07d-dfbd0a688004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) *2)\n",
    "\n",
    "\n",
    "def build_many_one_lstm():\n",
    "    inputs = Input(shape=(20, 1))\n",
    "    lstm_1 = lstm_layer(inputs)\n",
    "    outputs = Dense(units=1, activation=double_tanh)(lstm_1)\n",
    "    return keras.Model(inputs, outputs, name=f\"lstm1_fc1{lstm_hyper_param}\")\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "lstm_model = build_many_one_lstm()\n",
    "lstm_model.summary()\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=opt, metrics=lstm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2b47f-ad31-431f-8ded-1e2acf98d7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_csv_path = res_dir/f'{output_file_name}{lstm_hyper_param}_lstm_evaluation.csv'\n",
    "res_csv_path.touch(exist_ok=True)\n",
    "with open(res_csv_path, 'r+') as f:\n",
    "    if not f.read():\n",
    "        f.write(\"epoch,TRAIN_MSE,DEV_MSE,TEST1_MSE,TEST2_MSE,TRAIN_MAE,DEV_MAE,TEST1_MAE,TEST2_MAE\")\n",
    "\n",
    "res_df = pd.read_csv(res_csv_path)\n",
    "saved_model_list = [int(p.stem[p.stem.find(\"epoch\")+len(\"epoch\"):]) for p in model_dir.glob('*.h5')]\n",
    "epoch_start = max(saved_model_list) if saved_model_list else 1\n",
    "\n",
    "try:\n",
    "    for epoch_num in tqdm(range(epoch_start, max_epoch)):\n",
    "        if epoch_num > 1:\n",
    "            lstm_model = load_model(model_dir/f\"{output_file_name}{lstm_hyper_param}-epoch{epoch_num - 1}.h5\", custom_objects={'double_tanh':double_tanh})\n",
    "\n",
    "        save_model = ModelCheckpoint(model_dir/f\"{output_file_name}{lstm_hyper_param}-epoch{epoch_num}.h5\",\n",
    "                                     monitor='loss', verbose=1, mode='min', save_best_only=False)\n",
    "        lstm_model.fit(lstm_train_X, lstm_train_Y, epochs=1, batch_size=batch_size, callbacks=[model_log, save_model], shuffle=True, verbose=0)\n",
    "\n",
    "        # test the model\n",
    "        score_train = lstm_model.evaluate(lstm_train_X, lstm_train_Y)\n",
    "        score_dev = lstm_model.evaluate(lstm_dev_X, lstm_dev_Y)\n",
    "        score_test1 = lstm_model.evaluate(lstm_test1_X, lstm_test1_Y)\n",
    "        score_test2 = lstm_model.evaluate(lstm_test2_X, lstm_test2_Y)\n",
    "        metrics_mse_ind = lstm_metrics.index('mse') + 1  # need to plus one, because first term of lstm_model.evaluate() is loss\n",
    "        metrics_mae_ind = lstm_metrics.index('mae') + 1  # need to plus one, because first term of lstm_model.evaluate() is loss\n",
    "        res_each_epoch_df = pd.DataFrame(np.array([epoch_num, score_train[metrics_mse_ind], score_dev[metrics_mse_ind],\n",
    "                                                   score_test1[metrics_mse_ind], score_test2[metrics_mse_ind],\n",
    "                                                   score_train[metrics_mae_ind], score_dev[metrics_mae_ind],\n",
    "                                                   score_test1[metrics_mae_ind], score_test2[metrics_mae_ind]]).reshape(-1, 9),\n",
    "                                         columns=[\"epoch\", \"TRAIN_MSE\", \"DEV_MSE\", \"TEST1_MSE\",\n",
    "                                                  \"TEST2_MSE\", \"TRAIN_MAE\", \"DEV_MAE\",\n",
    "                                                  \"TEST1_MAE\", \"TEST2_MAE\"])\n",
    "        res_df = pd.concat([res_df, res_each_epoch_df])\n",
    "        if (res_df.shape[0] % 100) == 0:\n",
    "            res_df.to_csv(res_csv_path, index=False)  # insurance for ãfinallyã part doesent'work\n",
    "except Exception as e:\n",
    "    error_class = e.__class__.__name__  # åå¾é¯èª¤é¡å\n",
    "    detail = e.args[0]  # åå¾è©³ç´°å§å®¹\n",
    "    cl, exc, tb = sys.exc_info()  # åå¾Call Stack\n",
    "    last_call_stack = traceback.extract_tb(tb)[-1]  # åå¾Call Stackçæå¾ä¸ç­è³æ\n",
    "    file_name = last_call_stack[0]  # åå¾ç¼ççæªæ¡åç¨±\n",
    "    line_num = last_call_stack[1]  # åå¾ç¼ççè¡è\n",
    "    func_name = last_call_stack[2]  # åå¾ç¼ççå½æ¸åç¨±\n",
    "    err_msg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(file_name, line_num, func_name, error_class, detail)\n",
    "    logging.error(err_msg)\n",
    "else:\n",
    "    pass\n",
    "finally:\n",
    "    res_df.to_csv(res_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
