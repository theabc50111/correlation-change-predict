{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "333f12fa-b283-4785-9ff3-f92ee7611217",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [4254357447.py] {'CORR_STRIDE': 1, 'CORR_WINDOW': 10, 'DATA_DIV_STRIDE': 20, 'MAX_DATA_DIV_START_ADD': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.78 s (started: 2023-02-21 03:13:11 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from itertools import product, islice\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch.nn import Linear, GRU, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GINConv, global_mean_pool, global_add_pool, summary\n",
    "# from torchsummary import summary\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"/workspace/correlation-change-predict/ywt_library\")\n",
    "import data_module\n",
    "from data_module import data_gen_cfg, gen_corr_mat_thru_t\n",
    "\n",
    "\n",
    "with open('../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(format='%(levelname)-8s [%(filename)s] %(message)s',\n",
    "                    level=logging.INFO)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# logger_list = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "# loggin.debug(logger_list)\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.debug(pformat(data_cfg, indent=1, width=100, compact=True))\n",
    "logging.info(pformat(data_gen_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9618165-42eb-4b2b-b691-f18bbdb85b6c",
   "metadata": {},
   "source": [
    "## Data implement & output setting & testset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b164ae7b-db47-49ce-aae4-b8a80eb74f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [4194121536.py] ===== file_name basis:sp500_20082017_corr_ser_reg_corr_mat_hrchy_11_cluster-train_train =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.27 ms (started: 2023-02-21 03:13:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# data implement setting\n",
    "data_implement = \"SP500_20082017_CORR_SER_REG_CORR_MAT_HRCHY_11_CLUSTER\"  # watch options by operate: logging.info(data_cfg[\"DATASETS\"].keys())\n",
    "# train set setting\n",
    "train_items_setting = \"-train_train\"  # -train_train|-train_all\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + train_items_setting\n",
    "# setting of output files\n",
    "save_model_info = False\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc682cc-dc75-47e6-be5e-8f2b818e06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 761 µs (started: 2023-02-21 03:13:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "s_l, w_l = data_gen_cfg[\"CORR_STRIDE\"], data_gen_cfg[\"CORR_WINDOW\"]\n",
    "graph_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-graph_data\"\n",
    "model_dir = Path(f'./save_models/{output_file_name}/corr_s{s_l}_w{w_l}')\n",
    "model_log_dir = Path(f'./save_models/{output_file_name}/corr_s{s_l}_w{w_l}/train_logs/')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_log_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cb433-a7a3-421f-8642-0d42d5c715b9",
   "metadata": {},
   "source": [
    "## model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4a6e4e-e361-4b37-be10-8943c077c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 631 µs (started: 2023-02-21 03:13:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "gin_enc_cfg = {\"num_gin_layers\": 3,  # range:1~n, for GIN after the second layer,\n",
    "               \"gin_dim_h\": 3,\n",
    "              }\n",
    "data_loader_cfg = {\"tr_loader_batch_size\": 4,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"val_loader_batch_size\": 4,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"test_loader_batch_size\": 4,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                  }\n",
    "mts_corr_ad_cfg = {\"gru_layers\": 5,  # range:1~n, for gru\n",
    "                   \"gru_dim_out\": 8,\n",
    "                   }\n",
    "mts_corr_ad_cfg[\"dim_out\"] = gin_enc_cfg[\"num_gin_layers\"] *  gin_enc_cfg[\"gin_dim_h\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904302d0-3d2b-4f4d-8996-1bbf468b3083",
   "metadata": {},
   "source": [
    "## Load Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d1b5110-24fa-4739-b56a-b4f336f518d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO     [1790863446.py] graph_arr.shape:(2497, 66, 66)\n",
      "INFO     [1790863446.py] data.num_node_features: 1; data.num_edges: 4356; data.num_edge_features: 1; data.is_undirected: True; \n",
      "INFO     [1790863446.py] data.x.shape: torch.Size([66, 1]); data.y.x.shape: torch.Size([66, 1]); data.edge_index.shape: torch.Size([2, 4356]); data.edge_attr.shape: torch.Size([4356, 1])\n",
      "INFO     [1790863446.py] Training set   = 2246 graphs\n",
      "INFO     [1790863446.py] Validation set = 125 graphs\n",
      "INFO     [1790863446.py] Test set       = 125 graphs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.07 s (started: 2023-02-21 03:13:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "graph_arr = np.load(graph_data_dir/f\"corr_s{s_l}_w{w_l}_graph.npy\")  # each graph consist of 66 node & 66^2 edges\n",
    "logging.info(f\"graph_arr.shape:{graph_arr.shape}\")\n",
    "graph_time_step = graph_arr.shape[0] - 1  # the graph of last \"t\" can't be used as train data\n",
    "node_attr = torch.tensor(np.zeros((graph_arr.shape[1], 1)), dtype=torch.float32)  # each node has only one attribute\n",
    "edge_index = torch.tensor(list(product(range(graph_arr.shape[1]), repeat=2)))\n",
    "dataset = []\n",
    "for g_t in range(graph_time_step):\n",
    "    edge_attr = torch.tensor(np.hstack(graph_arr[g_t]).reshape(-1, 1), dtype=torch.float32)\n",
    "    edge_attr_next_t = torch.tensor(np.hstack(graph_arr[g_t+1]).reshape(-1, 1), dtype=torch.float32)\n",
    "    data_y = Data(x=node_attr, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr_next_t)\n",
    "    data = Data(x=node_attr, y=data_y, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr)\n",
    "    dataset.append(data)\n",
    "else:\n",
    "    #mts_corr_ad_cfg[\"dim_out\"] = data.y.shape[0]  # if the input of loss-function graphs instead of graphs' embedding\n",
    "    gin_enc_cfg[\"num_node_features\"] = data.num_node_features\n",
    "    logging.info(f\"data.num_node_features: {data.num_node_features}; data.num_edges: {data.num_edges}; data.num_edge_features: {data.num_edge_features}; data.is_undirected: {data.is_undirected()}; \")\n",
    "    logging.info(f\"data.x.shape: {data.x.shape}; data.y.x.shape: {data.y.x.shape}; data.edge_index.shape: {data.edge_index.shape}; data.edge_attr.shape: {data.edge_attr.shape}\")\n",
    "\n",
    "# Create training, validation, and test sets\n",
    "train_dataset = dataset[:int(len(dataset)*0.9)]\n",
    "val_dataset   = dataset[int(len(dataset)*0.9):int(len(dataset)*0.95)]\n",
    "test_dataset  = dataset[int(len(dataset)*0.95):]\n",
    "\n",
    "# Create mini-batches\n",
    "train_loader = DataLoader(train_dataset, batch_size = data_loader_cfg[\"tr_loader_batch_size\"], shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size = data_loader_cfg[\"val_loader_batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = data_loader_cfg[\"test_loader_batch_size\"], shuffle=False)\n",
    "\n",
    "# show info\n",
    "logging.info(f'Training set   = {len(train_dataset)} graphs')\n",
    "logging.info(f'Validation set = {len(val_dataset)} graphs')\n",
    "logging.info(f'Test set       = {len(test_dataset)} graphs')\n",
    "logging.debug('Train loader:')\n",
    "for i, subgraph in enumerate(train_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph {i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "logging.debug('Validation loader:')\n",
    "for i, subgraph in enumerate(val_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph{i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "logging.debug('Test loader:')\n",
    "for i, subgraph in enumerate(test_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph{i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "logging.debug('Peeking Train data:')\n",
    "data_x_nodes = next(iter(train_loader)).x.reshape(train_loader.batch_size, -1)\n",
    "data_x_edges = next(iter(train_loader)).edge_attr.reshape(train_loader.batch_size, -1)\n",
    "data_y_nodes = torch.cat([y.x for y in next(iter(train_loader)).y]).reshape(train_loader.batch_size, -1)\n",
    "data_y_edges = torch.cat([y.edge_attr for y in next(iter(train_loader)).y]).reshape(train_loader.batch_size, -1)\n",
    "for i in range(data_loader_cfg[\"tr_loader_batch_size\"]):\n",
    "    logging.debug(f\"\\n batch0_x{i}.shape: {data_x_nodes[i].shape} \\n batch0_x{i}[:5]:{data_x_nodes[i][:5]}\")\n",
    "    logging.debug(f\"\\n batch0_x{i}_edges.shape: {data_x_edges[i].shape} \\n batch0_x{i}_edges[:5]:{data_x_edges[i][:5]}\")\n",
    "    logging.debug(f\"\\n batch0_y{i}.shape: {data_y_nodes[i].shape} \\n batch0_y{i}[:5]:{data_y_nodes[i][:5]}\")\n",
    "    logging.debug(f\"\\n batch0_y{i}_edges.shape: {data_y_edges[i].shape} \\n batch0_y{i}_edges[:5]:{data_y_edges[i][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77157a0c-3ff4-4e6a-a5e1-a32dfdc0b6af",
   "metadata": {},
   "source": [
    "## Multi-Dimension Time-Series Correlation Anomly Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e5e2b9-8ec8-4d7d-b670-f157597aa1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.88 ms (started: 2023-02-21 03:13:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "class GinEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    num_node_features: number of features per node in the graph, in this model every node has same size of features \n",
    "    gin_dim_h: output size of hidden layer of GINconv\n",
    "    gru_layers: Number of recurrent layers of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features:int, num_gin_layers:int, gin_dim_h:int, **kwargs):\n",
    "        super(GinEncoder, self).__init__()\n",
    "        self.num_gin_layers = num_gin_layers\n",
    "        self.gin_convs = torch.nn.ModuleList()\n",
    "        self.gin_dim_h = gin_dim_h\n",
    "\n",
    "        for i in range(num_gin_layers):\n",
    "            if i:\n",
    "                nn = Sequential(Linear(gin_dim_h, gin_dim_h),\n",
    "                                BatchNorm1d(gin_dim_h), ReLU(),\n",
    "                                Linear(gin_dim_h, gin_dim_h), ReLU())\n",
    "            else:\n",
    "                nn = Sequential(Linear(num_node_features, gin_dim_h),\n",
    "                                BatchNorm1d(gin_dim_h), ReLU(),\n",
    "                                Linear(gin_dim_h, gin_dim_h), ReLU())\n",
    "            self.gin_convs.append(GINConv(nn))\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch_node_id):\n",
    "        # Node embeddings\n",
    "        nodes_emb_layers = []\n",
    "        for i in range(self.num_gin_layers):\n",
    "            if i:\n",
    "                nodes_emb = self.gin_convs[i](nodes_emb, edge_index)\n",
    "            else:\n",
    "                nodes_emb = self.gin_convs[i](x, edge_index)  # the shape of nodes_embeds: [batch_size*num_nodes, gin_dim_h] \n",
    "            nodes_emb_layers.append(nodes_emb)\n",
    "\n",
    "        # Graph-level readout\n",
    "        nodes_emb_pools = [global_add_pool(nodes_emb, batch_node_id) for nodes_emb in nodes_emb_layers]  # the shape of global_add_pool(nodes_emb, batch_node_id): [batch_size, gin_dim_h]\n",
    "                                                                                                         # global_add_pool : make a super-node to represent the graph\n",
    "        # Concatenate and form the graph embeddings\n",
    "        graph_embeds = torch.cat(nodes_emb_pools, dim=1)  # the shape of graph_embeds: [batch_size, num_layers*gin_dim_h]\n",
    "\n",
    "        return graph_embeds\n",
    "\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, batch_node_id):\n",
    "        with torch.no_grad():\n",
    "            graph_embeds = self.forward(x, edge_index, batch_node_id).reshape(-1)\n",
    "\n",
    "        return graph_embeds\n",
    "\n",
    "class MTSCorrAD(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    gru_dim_out: The number of output size of GRU and features in the hidden state h of GRU\n",
    "    dim_out: The number of output size of MTSCorrAD model\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_encoder:torch.nn.Module, gru_layers:int, gru_dim_out:int, dim_out:int, **kwargs):\n",
    "        super(MTSCorrAD, self).__init__()\n",
    "        self.graph_encoder = GinEncoder(**gin_enc_cfg).to(\"cuda\")\n",
    "        gru_input_size = self.graph_encoder.num_gin_layers * self.graph_encoder.gin_dim_h  # the input size of GRU depend on the number of layers of GINconv\n",
    "        self.gru1 = GRU(gru_input_size, gru_dim_out, gru_layers)\n",
    "        self.lin1 = Linear(gru_dim_out, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch_node_id):\n",
    "        # Inter-series modeling\n",
    "        graph_embeds = self.graph_encoder(x, edge_index, batch_node_id)\n",
    "\n",
    "        # Temporal Modeling\n",
    "        gru_output, gru_hn = self.gru1(graph_embeds)  # regarding batch_size as time-steps(sequence length) by using \"unbatched\" input\n",
    "        graph_embed_pred = self.lin1(gru_output[-1])  # gru_output[-1] => only take last time-step\n",
    "\n",
    "        return graph_embed_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eacc3a-f1c5-41e2-8f18-4e7696cf1a0e",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c94e7fe-e9ac-4d48-90aa-d6029a7d1ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 416 µs (started: 2023-02-21 03:13:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def barlo_twins_loss(pred: torch.Tensor, target: torch.Tensor):\n",
    "    assert pred.shape == target.shape, \"The shape of prediction and target aren't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ac20a-3a94-4d74-9452-e15261ffc4a8",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946d826a-0dc6-47dd-a74b-738bd7d09669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/1 [00:00<?, ?it/s]INFO     [3342465190.py] \n",
      "Number of graphs:2 in No.561 batch, the model structure:\n",
      "MTSCorrAD(\n",
      "  (graph_encoder): GinEncoder(\n",
      "    (gin_convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=3, out_features=3, bias=True)\n",
      "        (4): ReLU()\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=3, out_features=3, bias=True)\n",
      "        (4): ReLU()\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=3, out_features=3, bias=True)\n",
      "        (1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "        (3): Linear(in_features=3, out_features=3, bias=True)\n",
      "        (4): ReLU()\n",
      "      ))\n",
      "    )\n",
      "  )\n",
      "  (gru1): GRU(9, 8, num_layers=5)\n",
      "  (lin1): Linear(in_features=8, out_features=9, bias=True)\n",
      ")\n",
      "====================================================================================================\n",
      "+------------------------------+----------------------------+----------------+----------+\n",
      "| Layer                        | Input Shape                | Output Shape   | #Param   |\n",
      "|------------------------------+----------------------------+----------------+----------|\n",
      "| MTSCorrAD                    | [132, 1], [2, 8712], [132] | [9]            | 2,349    |\n",
      "| ├─(graph_encoder)GinEncoder  | [132, 1], [2, 8712], [132] | [2, 9]         | 84       |\n",
      "| │    └─(gin_convs)ModuleList | --                         | --             | 84       |\n",
      "| │    │    └─(0)GINConv       | [132, 1], [2, 8712]        | [132, 3]       | 24       |\n",
      "| │    │    └─(1)GINConv       | [132, 3], [2, 8712]        | [132, 3]       | 30       |\n",
      "| │    │    └─(2)GINConv       | [132, 3], [2, 8712]        | [132, 3]       | 30       |\n",
      "| ├─(gru1)GRU                  | [2, 9]                     | [2, 8], [5, 8] | 2,184    |\n",
      "| ├─(lin1)Linear               | [8]                        | [9]            | 81       |\n",
      "+------------------------------+----------------------------+----------------+----------+\n",
      "INFO     [3342465190.py] Epoch   0 | Train Loss: 84.60605 | Val Loss: 2083.93945 \n",
      "100% 1/1 [00:05<00:00,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.99 s (started: 2023-02-21 03:13:14 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model:torch.nn.Module, train_loader:torch_geometric.loader.dataloader.DataLoader,\n",
    "          val_loader:torch_geometric.loader.dataloader.DataLoader, optimizer, criterion, epochs:int =5, show_model_info=False):\n",
    "    best_model_info = {\"epochs\": epochs,\n",
    "                       \"train_batch\": train_loader.batch_size,\n",
    "                       \"val_batch\": val_loader.batch_size,\n",
    "                       \"optimizer\": optimizer.__str__(),\n",
    "                       \"criterion\": criterion.__str__(),\n",
    "                       \"graph_enc_w_grad_history\": [],\n",
    "                       \"min_val_loss\": float('inf'),\n",
    "                       \"train_loss_history\": [],\n",
    "                       \"val_loss_history\": [],\n",
    "                      }\n",
    "    model_num_layers = sum(1 for _ in model.parameters())\n",
    "    graph_enc_num_layers =  sum(1 for _ in model.graph_encoder.parameters())\n",
    "    graph_enc_w_grad_after = 0\n",
    "    for epoch_i in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        # Train on batches\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            data.to(\"cuda\")\n",
    "            x, x_edge_index, x_batch_node_id, x_edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "            y, y_edge_index, y_batch_node_id, y_edge_attr = data.y[-1].x, data.y[-1].edge_index, torch.zeros(data.y[-1].x.shape[0], dtype=torch.int64).to(\"cuda\"), data.y[-1].edge_attr  # only take y of x with last time-step on training\n",
    "            optimizer.zero_grad()\n",
    "            graph_embeds_pred = model(x, x_edge_index, x_batch_node_id)\n",
    "            y_graph_embeds = model.graph_encoder.get_embeddings(y, y_edge_index, y_batch_node_id)\n",
    "            loss =  criterion(graph_embeds_pred, y_graph_embeds)\n",
    "            train_loss += loss / len(train_loader)\n",
    "            loss.backward()\n",
    "            graph_enc_w_grad_after += sum(sum(torch.abs(torch.reshape(p.grad if p.grad!=None else torch.zeros((1,)).to('cuda'), (-1,)))) for p in islice(model.graph_encoder.parameters(), 0, graph_enc_num_layers))  # sums up in each batch\n",
    "            optimizer.step()\n",
    "\n",
    "        # Check if graph_encoder.parameters() have been updated\n",
    "        assert graph_enc_w_grad_after>0, f\"After loss.backward(), Sum of MainModel.graph_encoder weights in epoch_{epoch_i}:{graph_enc_w_grad_after}\"\n",
    "\n",
    "        # Validation\n",
    "        val_loss = test(model, val_loader, criterion)\n",
    "\n",
    "        # record training history\n",
    "        best_model_info[\"train_loss_history\"].append(train_loss.item())\n",
    "        best_model_info[\"val_loss_history\"].append(val_loss.item())\n",
    "        best_model_info[\"graph_enc_w_grad_history\"].append(graph_enc_w_grad_after.item())\n",
    "        # record training history and save best model\n",
    "        if val_loss<best_model_info[\"min_val_loss\"]:\n",
    "            best_model = model\n",
    "            best_model_info[\"best_val_epoch\"] = epoch_i\n",
    "            best_model_info[\"min_val_loss\"] = val_loss.item()\n",
    "\n",
    "        # observe model info in console\n",
    "        if show_model_info and epoch_i==0:\n",
    "            best_model_info[\"model_structure\"] = model.__str__() + \"\\n\" + \"=\"*100 + \"\\n\" + summary(model, data.x, data.edge_index, data.batch, max_depth=20).__str__()\n",
    "            logging.info(f\"\\nNumber of graphs:{data.num_graphs} in No.{batch_i} batch, the model structure:\\n{best_model_info['model_structure']}\")\n",
    "        if(epoch_i % 10 == 0):  # show metrics every 10 epochs\n",
    "            logging.info(f\"Epoch {epoch_i:>3} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} \")\n",
    "\n",
    "    return best_model, best_model_info\n",
    "\n",
    "\n",
    "def test(model:torch.nn.Module, loader:torch_geometric.loader.dataloader.DataLoader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_i, data in enumerate(loader):\n",
    "            data.to(\"cuda\")\n",
    "            x, x_edge_index, x_batch_node_id, x_edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "            y, y_edge_index, y_batch_node_id, y_edge_attr = data.y[-1].x, data.y[-1].edge_index, torch.zeros(data.y[-1].x.shape[0], dtype=torch.int64).to(\"cuda\"), data.y[-1].edge_attr  # only take y of x with last time-step on training\n",
    "            graph_embeds_pred = model(x, x_edge_index, x_batch_node_id)\n",
    "            y_graph_embeds = model.graph_encoder.get_embeddings(y, y_edge_index, y_batch_node_id)\n",
    "            loss = criterion(graph_embeds_pred, y_graph_embeds)\n",
    "            # test_loss += loss / len(loader)\n",
    "            test_loss += loss\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "def save_model(model:torch.nn.Module, model_info:dict, data_gen_cfg:dict):\n",
    "    e_i = model_info[\"best_val_epoch\"]\n",
    "    t = datetime.strftime(datetime.now(),\"%Y%m%d%H%M%S\")\n",
    "    torch.save(model, model_dir/f\"epoch_{e_i}-{t}.pt\")\n",
    "    with open(model_log_dir/f\"epoch_{e_i}-{t}.json\",\"w\") as f:\n",
    "        print(model_info)\n",
    "        json_str = json.dumps(model_info)\n",
    "        f.write(json_str)\n",
    "    logging.info(f\"model has been saved in:{model_dir}\")\n",
    "\n",
    "gin_encoder = GinEncoder(**gin_enc_cfg).to(\"cuda\")\n",
    "mts_corr_ad_cfg[\"graph_encoder\"] = gin_encoder\n",
    "model =  MTSCorrAD(**mts_corr_ad_cfg).to(\"cuda\")\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model, model_info = train(model, train_loader, val_loader, optimizer, criterion, epochs=1, show_model_info=True)\n",
    "if save_model_info:\n",
    "    save_model(model, model_info, data_gen_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
