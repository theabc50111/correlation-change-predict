{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f12fa-b283-4785-9ff3-f92ee7611217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "import logging\n",
    "from pprint import pformat\n",
    "from itertools import product\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "from torch.nn import Linear, GRU, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GINConv, global_mean_pool, global_add_pool, summary\n",
    "# from torchsummary import summary\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"/workspace/correlation-change-predict/ywt_library\")\n",
    "import data_generation\n",
    "from data_generation import data_gen_cfg, gen_corr_dist_mat\n",
    "from stl_decompn import stl_decompn\n",
    "from corr_property import calc_corr_ser_property\n",
    "\n",
    "\n",
    "with open('../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# logger_list = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "# loggin.debug(logger_list)\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.debug(pformat(data_cfg, indent=1, width=100, compact=True))\n",
    "logging.info(pformat(data_gen_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9618165-42eb-4b2b-b691-f18bbdb85b6c",
   "metadata": {},
   "source": [
    "## Data implement & output setting & testset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164ae7b-db47-49ce-aae4-b8a80eb74f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data implement setting\n",
    "data_implement = \"SP500_20082017_CORR_SER_REG_CORR_MAT_HRCHY_11_CLUSTER\"  # watch options by operate: logging.info(data_cfg[\"DATASETS\"].keys())\n",
    "# train set setting\n",
    "train_items_setting = \"-train_train\"  # -train_train|-train_all\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + train_items_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc682cc-dc75-47e6-be5e-8f2b818e06b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-graph_data\"\n",
    "model_dir = Path('./save_models/')\n",
    "model_log_dir = Path('./save_models/train_logs/')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_log_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cb433-a7a3-421f-8642-0d42d5c715b9",
   "metadata": {},
   "source": [
    "## model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a6e4e-e361-4b37-be10-8943c077c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "gin_enc_cfg = {\"num_gin_layers\": 3,  # range:1~n, for GIN after the second layer\n",
    "               \"gin_dim_h\": 24,\n",
    "              }\n",
    "mts_corr_ad_cfg = {\"tr_loader_batch_size\": 12,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"val_loader_batch_size\": 4,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"test_loader_batch_size\": 4,  # each graph contains 5 days correlation, so 4 graphs means a month, 12 graphs means a quarter\n",
    "                   \"gru_layers\": 1,  # range:1~n, for gru\n",
    "                   \"gru_dim_out\": 32,\n",
    "                   }\n",
    "mts_corr_ad_cfg[\"dim_out\"] = gin_enc_cfg[\"num_gin_layers\"] *  gin_enc_cfg[\"gin_dim_h\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904302d0-3d2b-4f4d-8996-1bbf468b3083",
   "metadata": {},
   "source": [
    "## Load Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b5110-24fa-4739-b56a-b4f336f518d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph_arr = np.load(graph_data_dir/f\"corr_calc_reg-corr_graph.npy\")  # each graph consist of 66 node & 66^2 edges\n",
    "logging.info(f\"graph_arr.shape:{graph_arr.shape}\")\n",
    "graph_time_step = graph_arr.shape[0] - 1  # the graph of last \"t\" can't be used as train data\n",
    "node_attr = torch.tensor(np.zeros((graph_arr.shape[1], 1)), dtype=torch.float32)  # each node has only one attribute\n",
    "edge_index = torch.tensor(list(product(range(graph_arr.shape[1]), repeat=2)))\n",
    "dataset = []\n",
    "for g_t in range(graph_time_step):\n",
    "    edge_attr = torch.tensor(np.hstack(graph_arr[g_t]).reshape(-1, 1), dtype=torch.float32)\n",
    "    edge_attr_next_t = torch.tensor(np.hstack(graph_arr[g_t+1]).reshape(-1, 1), dtype=torch.float32)\n",
    "    data_y = Data(x=node_attr, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr_next_t)\n",
    "    data = Data(x=node_attr, y=data_y, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr)\n",
    "    dataset.append(data)\n",
    "else:\n",
    "    #mts_corr_ad_cfg[\"dim_out\"] = data.y.shape[0]  # if the input of loss-function graphs instead of graphs' embedding\n",
    "    gin_enc_cfg[\"num_node_features\"] = data.num_node_features\n",
    "    logging.info(f\"data.num_node_features: {data.num_node_features}; data.num_edges: {data.num_edges}; data.num_edge_features: {data.num_edge_features}; data.is_undirected: {data.is_undirected()}; \")\n",
    "    logging.info(f\"data.x.shape: {data.x.shape}; data.y.x.shape: {data.y.x.shape}; data.edge_index.shape: {data.edge_index.shape}; data.edge_attr.shape: {data.edge_attr.shape}\")\n",
    "\n",
    "# Create training, validation, and test sets\n",
    "train_dataset = dataset[:int(len(dataset)*0.9)]\n",
    "val_dataset   = dataset[int(len(dataset)*0.9):int(len(dataset)*0.95)]\n",
    "test_dataset  = dataset[int(len(dataset)*0.95):]\n",
    "\n",
    "# Create mini-batches\n",
    "train_loader = DataLoader(train_dataset, batch_size = mts_corr_ad_cfg[\"tr_loader_batch_size\"], shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size = mts_corr_ad_cfg[\"val_loader_batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = mts_corr_ad_cfg[\"test_loader_batch_size\"], shuffle=False)\n",
    "\n",
    "# show info\n",
    "logging.info(f'Training set   = {len(train_dataset)} graphs')\n",
    "logging.info(f'Validation set = {len(val_dataset)} graphs')\n",
    "logging.info(f'Test set       = {len(test_dataset)} graphs')\n",
    "logging.debug('\\nTrain loader:')\n",
    "for i, subgraph in enumerate(train_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph {i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "logging.debug('\\nValidation loader:')\n",
    "for i, subgraph in enumerate(val_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph{i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "logging.debug('\\nTest loader:')\n",
    "for i, subgraph in enumerate(test_loader):\n",
    "    logging.debug(f' - Subgraph {i}: {subgraph} ; Subgraph{i}.num_graphs:{subgraph.num_graphs}')\n",
    "\n",
    "data_x_nodes = next(iter(train_loader)).x.reshape(mts_corr_ad_cfg[\"tr_loader_batch_size\"], -1)\n",
    "data_x_edges = next(iter(train_loader)).edge_attr.reshape(mts_corr_ad_cfg[\"tr_loader_batch_size\"], -1)\n",
    "data_y_nodes = torch.cat([y.x for y in next(iter(train_loader)).y]).reshape(mts_corr_ad_cfg[\"tr_loader_batch_size\"], -1)\n",
    "data_y_edges = torch.cat([y.edge_attr for y in next(iter(train_loader)).y]).reshape(mts_corr_ad_cfg[\"tr_loader_batch_size\"], -1)\n",
    "\n",
    "logging.debug('\\nPeeking Train data:')\n",
    "for i in range(12):\n",
    "    logging.debug(f\"\\n batch0_x{i}.shape: {data_x_nodes[i].shape} \\n batch0_x{i}[:5]:{data_x_nodes[i][:5]}\")\n",
    "    logging.debug(f\"\\n batch0_x{i}_edges.shape: {data_x_edges[i].shape} \\n batch0_x{i}_edges[:5]:{data_x_edges[i][:5]}\")\n",
    "    logging.debug(f\"\\n batch0_y{i}.shape: {data_y_nodes[i].shape} \\n batch0_y{i}[:5]:{data_y_nodes[i][:5]}\")\n",
    "    logging.debug(f\"\\n batch0_y{i}_edges.shape: {data_y_edges[i].shape} \\n batch0_y{i}_edges[:5]:{data_y_edges[i][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77157a0c-3ff4-4e6a-a5e1-a32dfdc0b6af",
   "metadata": {},
   "source": [
    "## Multi-Dimension Time-Series Correlation Anomly Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5e2b9-8ec8-4d7d-b670-f157597aa1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GinEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    num_node_features: number of features per node in the graph, in this model every node has same size of features \n",
    "    gin_dim_h: output size of hidden layer of GINconv\n",
    "    gru_layers: Number of recurrent layers of GRU\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features:int, num_gin_layers:int, gin_dim_h:int, **kwargs):\n",
    "        super(GinEncoder, self).__init__()\n",
    "        self.num_gin_layers = num_gin_layers\n",
    "        self.gin_convs = torch.nn.ModuleList()\n",
    "        self.gin_dim_h = gin_dim_h\n",
    "\n",
    "        for i in range(num_gin_layers):\n",
    "            if i:\n",
    "                nn = Sequential(Linear(gin_dim_h, gin_dim_h),\n",
    "                                BatchNorm1d(gin_dim_h), ReLU(),\n",
    "                                Linear(gin_dim_h, gin_dim_h), ReLU())\n",
    "            else:\n",
    "                nn = Sequential(Linear(num_node_features, gin_dim_h),\n",
    "                                BatchNorm1d(gin_dim_h), ReLU(),\n",
    "                                Linear(gin_dim_h, gin_dim_h), ReLU())\n",
    "            self.gin_convs.append(GINConv(nn))\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch_node_id):\n",
    "        # Node embeddings\n",
    "        nodes_emb_layers = []\n",
    "        for i in range(self.num_gin_layers):\n",
    "            if i:\n",
    "                nodes_emb = self.gin_convs[i](nodes_emb, edge_index)\n",
    "            else:\n",
    "                nodes_emb = self.gin_convs[i](x, edge_index)  # the shape of nodes_embeds: [batch_size*num_nodes, gin_dim_h] \n",
    "            nodes_emb_layers.append(nodes_emb)\n",
    "\n",
    "        # Graph-level readout\n",
    "        nodes_emb_pools = [global_add_pool(nodes_emb, batch_node_id) for nodes_emb in nodes_emb_layers]  # the shape of global_add_pool(nodes_emb, batch_node_id): [batch_size, gin_dim_h]\n",
    "                                                                                                         # global_add_pool : make a super-node to represent the graph\n",
    "        # Concatenate and form the graph embeddings\n",
    "        graph_embeds = torch.cat(nodes_emb_pools, dim=1)  # the shape of graph_embeds: [batch_size, num_layers*gin_dim_h]\n",
    "\n",
    "        return graph_embeds\n",
    "\n",
    "\n",
    "    def get_embeddings(self, x, edge_index, batch_node_id):\n",
    "        with torch.no_grad():\n",
    "            graph_embeds = self.forward(x, edge_index, batch_node_id).reshape(-1)\n",
    "\n",
    "        return graph_embeds\n",
    "\n",
    "class MTSCorrAD(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    gru_dim_out: The number of output size of GRU and features in the hidden state h of GRU\n",
    "    dim_out: The number of output size of MTSCorrAD model\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_encoder:torch.nn.Module, gru_layers:int, gru_dim_out:int, dim_out:int, **kwargs):\n",
    "        super(MTSCorrAD, self).__init__()\n",
    "        self.graph_encoder = graph_encoder\n",
    "        gru_input_size = self.graph_encoder.num_gin_layers * self.graph_encoder.gin_dim_h  # the input size of GRU depend on the number of layers of GINconv\n",
    "        self.gru1 = GRU(gru_input_size, gru_dim_out, gru_layers)\n",
    "        self.lin1 = Linear(gru_dim_out, dim_out)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch_node_id):\n",
    "        # Inter-series modeling\n",
    "        graph_embeds = self.graph_encoder(x, edge_index, batch_node_id)\n",
    "\n",
    "        # Temporal Modeling\n",
    "        gru_output, gru_hn = self.gru1(graph_embeds)  # regarding batch_size as time-steps(sequence length) by using \"unbatched\" input\n",
    "        graph_embed_pred = self.lin1(gru_output[-1])  # gru_output[-1] => only take last time-step\n",
    "\n",
    "        return graph_embed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d826a-0dc6-47dd-a74b-738bd7d09669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model:torch.nn.Module, train_loader, val_loader, optimizer, criterion, epochs=5, show_model_info=False):\n",
    "    best_model_info = {\"min_val_loss\": float('inf'),\n",
    "                       \"train_loss_history\": [],\n",
    "                       \"val_loss_history\": []\n",
    "                      }\n",
    "    for epoch_i in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        # Train on batches\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            data.to(\"cuda\")\n",
    "            x, x_edge_index, x_batch_node_id, x_edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "            y, y_edge_index, y_batch_node_id, y_edge_attr = data.y[-1].x, data.y[-1].edge_index, torch.zeros(data.y[-1].x.shape[0], dtype=torch.int64).to(\"cuda\"), data.y[-1].edge_attr  # only take y of x with last time-step on training\n",
    "            optimizer.zero_grad()\n",
    "            graph_embeds_pred = model(x, x_edge_index, x_batch_node_id)\n",
    "            y_graph_embeds = model.graph_encoder.get_embeddings(y, y_edge_index, y_batch_node_id)\n",
    "            loss =  criterion(graph_embeds_pred, y_graph_embeds)\n",
    "            train_loss += loss / len(train_loader)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if  show_model_info and epoch_i==0 and batch_i==0:\n",
    "                logging.info(model)\n",
    "                logging.info(summary(model, data.x, data.edge_index, data.batch, max_depth=20))\n",
    "\n",
    "        # Validation\n",
    "        val_loss = test(model, val_loader, criterion)\n",
    "\n",
    "        # save best model\n",
    "        if val_loss<best_model_info[\"min_val_loss\"]:\n",
    "            best_model = model\n",
    "            best_model_info[\"best_val_epoch\"] = epoch_i\n",
    "            best_model_info[\"min_val_loss\"] = val_loss.item()\n",
    "\n",
    "        best_model_info[\"train_loss_history\"].append(train_loss.item())\n",
    "        best_model_info[\"val_loss_history\"].append(val_loss.item())\n",
    "\n",
    "        # show metrics every 100 epochs\n",
    "        if(epoch_i % 100 == 0):\n",
    "            logging.info(f\"Epoch {epoch_i:>3} | Train Loss: {train_loss:.5f} | Val Loss: {val_loss:.5f} \")\n",
    "\n",
    "    return best_model, best_model_info\n",
    "\n",
    "\n",
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_i, data in enumerate(loader):\n",
    "            data.to(\"cuda\")\n",
    "            x, x_edge_index, x_batch_node_id, x_edge_attr = data.x, data.edge_index, data.batch, data.edge_attr\n",
    "            y, y_edge_index, y_batch_node_id, y_edge_attr = data.y[-1].x, data.y[-1].edge_index, torch.zeros(data.y[-1].x.shape[0], dtype=torch.int64).to(\"cuda\"), data.y[-1].edge_attr  # only take y of x with last time-step on training\n",
    "            graph_embeds_pred = model(x, x_edge_index, x_batch_node_id)\n",
    "            y_graph_embeds = model.graph_encoder.get_embeddings(y, y_edge_index, y_batch_node_id)\n",
    "            loss += criterion(graph_embeds_pred, y_graph_embeds) / len(loader)\n",
    "\n",
    "    return loss\n",
    "\n",
    "gin_encoder = GinEncoder(**gin_enc_cfg).to(\"cuda\")\n",
    "mts_corr_ad_cfg[\"graph_encoder\"] = gin_encoder\n",
    "model =  MTSCorrAD(**mts_corr_ad_cfg).to(\"cuda\")\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "model, model_info = train(model, train_loader, val_loader, optimizer, criterion, epochs=10, show_model_info=True)\n",
    "torch.save(model, model_dir/f\"{output_file_name}.pt\")\n",
    "with open(model_log_dir/\"model_train_log.json\",\"w\") as f:\n",
    "    json_str = json.dumps(model_info)\n",
    "    f.write(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567b7fb-d870-4776-a4e5-48dd1c72de7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
