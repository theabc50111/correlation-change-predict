{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2c178-aa69-4c4c-8ac1-90ba2563a03f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pformat, pprint\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from math import ceil\n",
    "from itertools import repeat, chain, product, cycle, combinations\n",
    "import traceback\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from seaborn import heatmap\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "logging.basicConfig(format='%(levelname)-8s [%(filename)s] %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bd00b-5f02-45ab-84f7-4f532d108249",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Draw the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55792e4-d68d-4b91-9626-816ab8d97d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def baseline_gru_tr_proc_est(log_path_list: list, condition_dict: dict, regexp_res: bool = False,  plot_pic:bool = True):\n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "        for log_path in log_path_list:\n",
    "            with open(log_path, \"r\") as source:\n",
    "                log_dict = json.load(source)\n",
    "\n",
    "            for k in log_dict.keys():\n",
    "                locals()[k] = log_dict[k]\n",
    "            corr_info = str(next(filter(lambda p: p.startswith(\"corr\"), log_path.parts)))\n",
    "            min_tr_loss = min(locals()[\"tr_loss_history\"])\n",
    "            min_tr_loss_edge_acc = locals()[\"tr_edge_acc_history\"][np.argmin(np.array(locals()[\"tr_loss_history\"]))]\n",
    "            max_tr_edge_acc = max(locals()[\"tr_edge_acc_history\"])\n",
    "            max_val_edge_acc = max(locals()[\"val_edge_acc_history\"])\n",
    "            model_struct_str = log_dict.get('model_structure')\n",
    "            record_fields = list(log_dict.keys()) + [\"corr_info\", \"min_tr_loss\", \"min_tr_loss_edge_acc\", \"max_tr_edge_acc\", \"max_val_edge_acc\"]\n",
    "            if regexp_res:\n",
    "                model_struct_info_fields = {\"gru_l\": \"\\(gru\\): GRU\\(\\d+, \\d+, num_layers=(?P<gru_l>\\d+).*\\)\",\n",
    "                                            \"gru_h\": \"\\(gru\\): GRU\\(\\d+, (?P<gru_h>\\d+), .+\\)\"}\n",
    "                if model_struct_str:\n",
    "                    for field, pattern in model_struct_info_fields.items():\n",
    "                        match = re.search(pattern, model_struct_str)\n",
    "                        if match:\n",
    "                            locals()[field] = match.group(field)\n",
    "                        else:\n",
    "                            locals()[field] = None\n",
    "                            logging.info(f\"Can't detect {field}\")\n",
    "                    else:\n",
    "                        locals()[\"gru_l\"] = 1 if not locals()[\"gru_l\"] else locals()[\"gru_l\"]\n",
    "                record_fields += list(model_struct_info_fields.keys())\n",
    "\n",
    "            assert not(set(condition_dict.keys()) - set(locals().keys())), \"one of condition_dict.keys() doesn't match the local variables if mts_corr_ad_est()\"\n",
    "            est_values_dict = locals()\n",
    "            filtered_dict = dict(filter(lambda x: est_values_dict[x[0]] == x[1], condition_dict.items()))\n",
    "            if filtered_dict == condition_dict:\n",
    "                main_title_str = (f\"{locals().get('corr_info')} with filt:{locals().get('filt_mode')}-{locals().get('filt_quan')} \"\n",
    "                                  f\"and batch_size({locals().get('batch_size')}) \"\n",
    "                                  f\"input to GRU with gru_l{locals().get('gru_l')}-gru_h{locals().get('gru_h')}\\n\"\n",
    "                                  f\"with drop: {locals().get('drop_p')} and loss_fns:{locals().get('loss_fns')}\\n\"\n",
    "                                  f\"min val-loss:{locals().get('min_val_loss'):8f} min tr-loss:{locals().get('min_tr_loss'):8f}\")\n",
    "                logging.info(f\"file_name:{log_path.parts[-1]}\")\n",
    "                logging.info(f\"file_path:{log_path.parts[2:-2]}\")\n",
    "                logging.info(f\"main_title_str:\\n{main_title_str}\")\n",
    "                comparison_dict = dict(filter(lambda x: x[0] in record_fields, locals().items()))\n",
    "                df = pd.concat([df, pd.DataFrame([comparison_dict])])\n",
    "                if plot_pic:\n",
    "                    pass\n",
    "                    plot_mts_corr_ad_tr_process(main_title=main_title_str, model_struct=model_struct_str, metrics_history={k:log_dict[k] for k in record_fields if \"history\" in k},\n",
    "                                                best_epoch=locals()['best_val_epoch'], batches_per_epoch=locals()['batches_per_epoch'])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            df = df.reindex([\"corr_info\", \"epochs\", \"batch_size\", \"gra_nodes_v_mode\", \"filt_mode\", \"filt_quan\", \"quan_discrete_bins\",\n",
    "                             \"custom_discrete_bins\", \"seq_len\", \"loss_fns\", \"loss_weight\", \"drop_pos\", \"drop_p\", \"gru_l\", \"gru_h\", \"decoder\",\n",
    "                             \"output_type\", \"output_bins\", \"target_mats_bins\", \"edge_acc_loss_atol\", \"two_ord_pred_prob_edge_accu_thres\", 'best_val_epoch',\n",
    "                             \"min_tr_loss\", \"min_tr_loss_edge_acc\", \"max_tr_edge_acc\", \"min_val_loss\", \"max_val_edge_acc\", \"min_val_loss_edge_acc\"], axis=1)\n",
    "            df = df.sort_values([\"batch_size\", \"seq_len\", \"gru_l\", \"gru_h\", \"drop_p\"], ascending=False)\n",
    "            df = df.reset_index(drop=True)\n",
    "            df.style.set_caption('Info of Baseline_GRU model with different hyperparameters')\n",
    "            pd.options.display.float_format = '{:.6f}'.format\n",
    "            pd.set_option('display.max_columns', None)\n",
    "            display(df)\n",
    "    except Exception as e:\n",
    "        error_class = e.__class__.__name__ #⬞取得錯誤類型\n",
    "        detail = e.args[0]  #⬞取得詳細內容\n",
    "        cl, exc, tb = sys.exc_info() #⬞取得Call⬞Stack\n",
    "        last_call_stack = traceback.extract_tb(tb)[-1] #⬞取得Call⬞Stack的最後一筆資料↵\n",
    "        file_name = last_call_stack[0] #⬞取得發生的檔案名稱↵\n",
    "        line_num = last_call_stack[1] #⬞取得發生的行號↵\n",
    "        func_name = last_call_stack[2] #⬞取得發生的函數名稱\n",
    "        err_msg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(file_name, line_num, func_name, error_class, detail)\n",
    "        logging.error(f\"file:{log_path.parts[-1]}, path:{log_path}\")\n",
    "        logging.error(f\"===\\n{err_msg}\")\n",
    "        logging.error(f\"===\\n{traceback.extract_tb(tb)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_mts_corr_ad_tr_process(main_title: str, model_struct: str, metrics_history: dict, best_epoch: int, batches_per_epoch: int):\n",
    "    max_batch = batches_per_epoch * len(metrics_history['tr_loss_history'])  # epochs == len(metrics_history['tr_loss'])\n",
    "    data_info_dict = [{\"sub_title\": 'train loss_history & edge_acc_history',\n",
    "                       \"data\": {'tr_loss_history': metrics_history['tr_loss_history'],\n",
    "                                'tr_edge_acc_history': metrics_history['tr_edge_acc_history']},\n",
    "                       \"xticks\": None,\n",
    "                       \"xlabel\": \"epochs\",\n",
    "                       \"double_y\": True},\n",
    "                      {\"sub_title\": 'val  loss_history & edge_acc_history',\n",
    "                       \"data\": {'val_loss_history': metrics_history['val_loss_history'],\n",
    "                                'val_edge_acc_history': metrics_history['val_edge_acc_history']},\n",
    "                       \"xticks\": None,\n",
    "                       \"xlabel\": \"epochs\",\n",
    "                       \"double_y\": True},\n",
    "                      # {\"sub_title\": 'train gradient_history',\n",
    "                      #  \"data\": {\"gru_gradient_history\": metrics_history['gru_gradient_history'],\n",
    "                      #           \"fc_gradient_history\": metrics_history['fc_gradient_history']},\n",
    "                      #  \"xticks\": None,\n",
    "                      #  \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": f\"model structure\",\n",
    "                       \"data\": str(model_struct)}]\n",
    "\n",
    "    # figrue settings\n",
    "    line_style = {\"linewidth\": 2, \"alpha\": 0.5}\n",
    "    axvline_style = {\"color\": 'k', \"linewidth\": 5, \"linestyle\": '--', \"alpha\": 0.3}\n",
    "    fig, axs = plt.subplot_mosaic(\"\"\"\n",
    "                                  ab\n",
    "                                  cc\n",
    "                                  \"\"\",\n",
    "                                  figsize=(30, 20), gridspec_kw={'hspace': 0.2, 'wspace': 0.3})\n",
    "    fig.suptitle(main_title, fontsize=30)\n",
    "\n",
    "    try:\n",
    "        for ax, data_plot in zip(axs.values(), data_info_dict):\n",
    "            ax.set_title(data_plot[\"sub_title\"], fontsize=30)\n",
    "            ax.yaxis.offsetText.set_fontsize(18)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "            if isinstance(data_plot[\"data\"], dict) and data_plot.get(\"double_y\"):\n",
    "                for i, key in enumerate(data_plot[\"data\"]):\n",
    "                    if i == 0:\n",
    "                        ax.plot(data_plot[\"data\"][key], label=key, **line_style)\n",
    "                        ax.set_ylabel(key, fontsize=24)\n",
    "                        ax.legend(fontsize=18)\n",
    "                    else:\n",
    "                        new_ax = ax.twinx()\n",
    "                        new_ax.plot(data_plot[\"data\"][key], label=key, color='r')\n",
    "                        new_ax.set_ylabel(key, color='r', fontsize=24)\n",
    "                        new_ax.legend(fontsize=18)\n",
    "                        new_ax.tick_params(axis='both', colors='r', which='major', labelsize=24)\n",
    "            elif isinstance(data_plot[\"data\"], dict):\n",
    "                [ax.plot(data_plot[\"data\"][key], label=key, **line_style) for key in data_plot[\"data\"]]\n",
    "                ax.legend(fontsize=18)\n",
    "            elif isinstance(data_plot[\"data\"], str):\n",
    "                ax.annotate(text=f\"{data_plot['data']}\",\n",
    "                            xy=(0.15, 0.5), bbox={'facecolor': 'green', 'alpha': 0.4, 'pad': 5},\n",
    "                            fontsize=20, fontfamily='monospace', xycoords='axes fraction', va='center')\n",
    "            else:\n",
    "                ax.plot(data_plot[\"data\"], **line_style)\n",
    "            if pos_tuple := data_plot.get(\"axvline\"):\n",
    "                for x_pos in pos_tuple:\n",
    "                    ax.axvline(x=x_pos, **axvline_style)\n",
    "            if xlabel := data_plot.get(\"xlabel\"):\n",
    "                ax.set_xlabel(xlabel, fontsize=24)\n",
    "            if t := data_plot.get(\"xticks\"):\n",
    "                ax.set_xticks(ticks=range(0, len(t[\"label\"])*t[\"intv\"], t[\"intv\"]), labels=t[\"label\"], rotation=45)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Encounter error when draw figure of {data_plot['sub_title']}\")\n",
    "        raise e\n",
    "\n",
    "    fig.tight_layout(rect=(0, 0, 0, 0))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87825fad-c2a3-4c17-af2d-7aa3edc0bf35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_gru_log_dir = Path(\"./save_models/class_baseline_gru_one_feature/archive/20230913/sp500_20112015_corr_ser_reg_std_corr_mat_negative_filtered-train_train/pearson/\")\n",
    "log_path_list1 = baseline_gru_log_dir.glob(\"./*[!deprecated][!archive][!.ipynb_checkpoints]*/train_logs/*[!.ipynb_checkpoints]*[.json]\")\n",
    "log_path_list2 = baseline_gru_log_dir.glob(\"./*[archive][!deprecated][!.ipynb_checkpoints]*/**/train_logs/*[!.ipynb_checkpoints]*[.json]\")\n",
    "log_path_list3 = baseline_gru_log_dir.glob(\"./**/train_logs/*[!.ipynb_checkpoints]*[.json]\")\n",
    "\n",
    "model_tr_summary_df = baseline_gru_tr_proc_est(log_path_list=log_path_list1, condition_dict={}, regexp_res=False, plot_pic=True)\n",
    "columns_containing_lists = model_tr_summary_df.apply(lambda col: isinstance(col.iloc[0], list)).iloc[list(model_tr_summary_df.apply(lambda col: isinstance(col.iloc[0], list)) == True)].index\n",
    "columns_not_containing_lists = model_tr_summary_df.columns.difference(columns_containing_lists)\n",
    "independent_variables_columns = model_tr_summary_df.loc[::, columns_not_containing_lists].nunique()[model_tr_summary_df.loc[::, columns_not_containing_lists].nunique() > 1].index\n",
    "control_variables_columns = model_tr_summary_df.loc[::, columns_not_containing_lists].nunique().index.difference(independent_variables_columns)\n",
    "for col in model_tr_summary_df.loc[::, columns_containing_lists]:\n",
    "    col_ser = model_tr_summary_df.loc[::, col].apply(lambda x:str(x))\n",
    "    if len(np.unique(col_ser)) > 1:\n",
    "        independent_variables_columns = independent_variables_columns.append(pd.Index([col]))\n",
    "    else:\n",
    "        control_variables_columns = control_variables_columns.append(pd.Index([col]))\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "independent_variables_tr_summary_df = model_tr_summary_df.loc[::, independent_variables_columns].sort_index(axis=1)\n",
    "control_variables_tr_summary_df = model_tr_summary_df.loc[0, control_variables_columns].sort_index(axis=0)\n",
    "display(control_variables_tr_summary_df)\n",
    "display(independent_variables_tr_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568f54d-ec64-4b97-a7cc-f8aefd206c4f",
   "metadata": {},
   "source": [
    "# Observe prediction and labels during training and validation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3aea6-8a83-46b5-8bac-eebec32b7d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs_baseline_gru_log_path = Path(\"./save_models/class_baseline_gru_without_self_corr/archive/20230908/sp500_20112015_corr_ser_reg_std_corr_mat_large_filtered_hrchy_10_cluster_label_last_v2-train_train/pearson/corr_s1_w50/train_logs/epoch_1499-20230908033654.json\")\n",
    "with open(obs_baseline_gru_log_path, \"r\") as f:\n",
    "    log = json.load(f)\n",
    "\n",
    "tr_preds_history = log['tr_preds_history']\n",
    "tr_labels_history = log['tr_labels_history']\n",
    "val_preds_history = log['val_preds_history']\n",
    "val_labels_history = log['val_labels_history']\n",
    "last_batch_size = len(tr_preds_history[0])\n",
    "graph_adj_mat_size = len(tr_preds_history[0][0])\n",
    "graph_adj_mat_items = [\"ADP\", \"APH\", \"MDT\", \"PAYX\"]  ## observe the index by operating `ython gen_corr_graph_data.py --data_implement <data_config.yaml['DATASETS'][DATASET_NAME]>`\n",
    "# graph_adj_mat_items = ['NEM', 'ETR', 'INCY']  ## observe the index by operating `ython gen_corr_graph_data.py --data_implement <data_config.yaml['DATASETS'][DATASET_NAME]>`\n",
    "num_epochs = len(tr_preds_history)\n",
    "num_obs_epochs = 5\n",
    "obs_epochs = np.linspace(0, num_epochs-1, num_obs_epochs, dtype=\"int\")\n",
    "obs_best_val_epoch = np.argmax(np.array(log['val_edge_acc_history']))\n",
    "obs_batch_idx = 10\n",
    "\n",
    "if math.sqrt(len(tr_preds_history[0][0])).is_integer():\n",
    "    num_nodes = int(math.sqrt(len(tr_preds_history[0][0])))\n",
    "    is_square_graph = True\n",
    "else:\n",
    "    num_nodes_minus_one = 1\n",
    "    while (num_nodes_minus_one**2 + num_nodes_minus_one)/2 != len(tr_preds_history[0][0]):  # arithmetic progression sum formula\n",
    "        num_nodes_minus_one += 1\n",
    "    num_nodes = num_nodes_minus_one+1\n",
    "    is_square_graph = False\n",
    "assert is_square_graph == (graph_adj_mat_size == num_nodes**2), \"when the graph is square graph, the size of graph should be num_nodes**2\"\n",
    "assert len(tr_preds_history) == len(tr_labels_history) and len(tr_preds_history) == len(val_preds_history) and len(tr_preds_history) == len(val_labels_history), \"length of {tr_preds_history, tr_labels_history, val_preds_history, val_labels_history} should be equal to num_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef83ec-a3ef-4f63-847d-d0e5e09e7582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert isinstance(tr_preds_history[0][0][0], int), \"Following operation require model's output are classification\"\n",
    "tr_preds_each_obs_epoch, tr_labels_each_obs_epoch, val_preds_each_obs_epoch, val_labels_each_obs_epoch = [None]*4\n",
    "obs_vars_list = [\"tr_preds_each_obs_epoch\", \"tr_labels_each_obs_epoch\", \"val_preds_each_obs_epoch\", \"val_labels_each_obs_epoch\"]\n",
    "history_record_list = [tr_preds_history, tr_labels_history, val_preds_history, val_labels_history]\n",
    "tr_obs_df = pd.DataFrame()\n",
    "val_obs_df = pd.DataFrame()\n",
    "for epoch_idx in obs_epochs:\n",
    "    for obs_var, history in zip(obs_vars_list, history_record_list):\n",
    "        if is_square_graph:\n",
    "            locals()[obs_var] = np.array(history[epoch_idx][obs_batch_idx]).reshape(num_nodes, num_nodes)\n",
    "        else:\n",
    "            tmp = np.zeros((num_nodes, num_nodes), dtype=\"int\")\n",
    "            ret_start = 0\n",
    "            for i in range(1, num_nodes+1):\n",
    "                ret_len = num_nodes-i\n",
    "                ret_end = ret_start+ret_len\n",
    "                tmp[i-1] = [0]*i+(history[epoch_idx][obs_batch_idx][ret_start:ret_end])\n",
    "                ret_start = ret_end\n",
    "            locals()[obs_var] = tmp\n",
    "\n",
    "    tr_obs_df_each_obs_epoch = pd.DataFrame(np.concatenate([tr_preds_each_obs_epoch, tr_labels_each_obs_epoch], axis=0), columns=graph_adj_mat_items, index=graph_adj_mat_items*2)\n",
    "    tr_obs_df = pd.concat([tr_obs_df, tr_obs_df_each_obs_epoch], axis=1)\n",
    "    val_obs_df_each_obs_epoch = pd.DataFrame(np.concatenate([val_preds_each_obs_epoch, val_labels_each_obs_epoch], axis=0), columns=graph_adj_mat_items, index=graph_adj_mat_items*2)\n",
    "    val_obs_df = pd.concat([val_obs_df, val_obs_df_each_obs_epoch], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "multi_level_col = pd.MultiIndex.from_product([[f\"epoch_{epoch_idx}\" for epoch_idx in obs_epochs], graph_adj_mat_items])\n",
    "multi_level_idx = pd.MultiIndex.from_product([[\"predictions\", \"labels\"], graph_adj_mat_items])\n",
    "tr_obs_df.columns = multi_level_col\n",
    "tr_obs_df.index = multi_level_idx\n",
    "val_obs_df.columns = multi_level_col\n",
    "val_obs_df.index = multi_level_idx\n",
    "h_line = pd.DataFrame([\"―\"]*tr_obs_df.shape[1], columns=[\"h_line\"], index=multi_level_col).T\n",
    "tr_obs_df = pd.concat([tr_obs_df.iloc[:num_nodes, ], h_line, tr_obs_df.iloc[num_nodes:, ]], axis=0)\n",
    "val_obs_df = pd.concat([val_obs_df.iloc[:num_nodes, ], h_line, val_obs_df.iloc[num_nodes:, ]], axis=0)\n",
    "for i in range(1, num_obs_epochs):\n",
    "    v_line_loc = i*num_nodes+(i-1)\n",
    "    tr_obs_df.insert(v_line_loc, f\"v_line_{i}\", [\"|\"]*tr_obs_df.shape[0])\n",
    "    val_obs_df.insert(v_line_loc, f\"v_line_{i}\", [\"|\"]*val_obs_df.shape[0])\n",
    "\n",
    "tr_obs_df = tr_obs_df.style.set_caption(f\"train prediction & labels of the graph in {obs_batch_idx}th of last_batch for epochs\").set_table_styles([{'selector': 'caption',\n",
    "                                                                                                                                         'props': [('color', 'red'),\n",
    "                                                                                                                                                   ('font-size', '24px')]}])\n",
    "val_obs_df = val_obs_df.style.set_caption(f\"Validation prediction & labels of the graph in {obs_batch_idx}th of last_batch for epochs\").set_table_styles([{'selector': 'caption',\n",
    "                                                                                                                                                'props': [('color', 'red'),\n",
    "                                                                                                                                                          ('font-size', '24px')]}])\n",
    "display(tr_obs_df)\n",
    "display(val_obs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb4a09-8024-40de-8cb2-88e0aa759953",
   "metadata": {},
   "source": [
    "## Observe the heatmap of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cc0d1b-9c8e-445e-91e7-fca92ac0cf54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_batch_size = len(val_preds_history[0])\n",
    "best_epoch_val_preds = np.array(val_preds_history[obs_best_val_epoch])\n",
    "best_epoch_val_labels = np.array(val_labels_history[obs_best_val_epoch])\n",
    "total_val_data_confusion_matrix = pd.DataFrame(confusion_matrix(best_epoch_val_labels.reshape(-1), best_epoch_val_preds.reshape(-1), labels=[0, 1, 2]), columns=range(-1,2), index=range(-1,2))\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.rcParams.update({'font.size': 44})\n",
    "ax = plt.gca()\n",
    "heatmap(total_val_data_confusion_matrix, annot=True, ax=ax, fmt='g')\n",
    "ax.set(xlabel=\"Prediction\", ylabel=\"Ground Truth\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "#pred_incorrect_mask = (best_epoch_val_preds != best_epoch_val_labels)\n",
    "#print(pred_incorrect_mask.sum(), best_epoch_val_preds.shape, best_epoch_val_preds.size)\n",
    "#print(best_epoch_val_preds[pred_incorrect_mask])\n",
    "#print(best_epoch_val_labels[pred_incorrect_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a73733b-853a-4e58-92a0-7cbf9c3f09c1",
   "metadata": {},
   "source": [
    "## Observe the correlation type distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d049a1-71a9-4abc-b1ee-887ebb5890e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "items_pairs = list(product(graph_adj_mat_items, repeat=2)) if is_square_graph else list(combinations(graph_adj_mat_items, 2))\n",
    "val_graph_corr_type_distribution = pd.DataFrame(columns=[\"preds\", \"labels\"])\n",
    "val_preds_graph_corr_type_pie_plot_ser = pd.Series()\n",
    "val_labels_graph_corr_type_pie_plot_ser = pd.Series()\n",
    "for pairs, graph_corr_type in zip(cycle([items_pairs]), product((-1, 0, 1), repeat=graph_adj_mat_size)):\n",
    "    val_preds_graph_corr_type_count = np.apply_along_axis(lambda x: all(x==graph_corr_type), 1, (best_epoch_val_preds-1)).sum()\n",
    "    val_labels_graph_corr_type_count = np.apply_along_axis(lambda x: all(x==graph_corr_type), 1, (best_epoch_val_labels-1)).sum()\n",
    "    val_graph_corr_type_distribution = pd.concat([val_graph_corr_type_distribution, pd.DataFrame([[val_preds_graph_corr_type_count, val_labels_graph_corr_type_count]], index=[str(graph_corr_type)], columns=[\"preds\", \"labels\"])])\n",
    "    if val_labels_graph_corr_type_count:\n",
    "        val_labels_graph_corr_type_pie_plot_ser = pd.concat([val_labels_graph_corr_type_pie_plot_ser, pd.Series([val_labels_graph_corr_type_count], index=[str(dict(zip(pairs, graph_corr_type)))])])\n",
    "    if val_preds_graph_corr_type_count:\n",
    "        val_preds_graph_corr_type_pie_plot_ser = pd.concat([val_preds_graph_corr_type_pie_plot_ser, pd.Series([val_preds_graph_corr_type_count], index=[str(dict(zip(pairs, graph_corr_type)))])])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(30, 20))\n",
    "ax[0].pie(val_labels_graph_corr_type_pie_plot_ser.values, labels=val_labels_graph_corr_type_pie_plot_ser.index, autopct='%1.1f%%', textprops={'fontsize': 20})\n",
    "ax[0].set_title(\"Val labels distribution\", fontsize=44)\n",
    "ax[1].pie(val_preds_graph_corr_type_pie_plot_ser.values, labels=val_preds_graph_corr_type_pie_plot_ser.index, autopct='%1.1f%%', textprops={'fontsize': 20})\n",
    "ax[1].set_title(\"Val preds distribution\", fontsize=44)\n",
    "plt.show()\n",
    "plt.close()\n",
    "val_graph_corr_type_distribution.index.name = str(items_pairs)\n",
    "display_mask = np.logical_or((val_graph_corr_type_distribution.loc[::, [\"preds\"]]!=0).values, (val_graph_corr_type_distribution.loc[::, [\"labels\"]]!=0).values)\n",
    "print(display_mask.shape)\n",
    "display(val_graph_corr_type_distribution.loc[display_mask, ::])\n",
    "display(val_graph_corr_type_distribution.iloc[:9, ::])\n",
    "val_graph_corr_type_distribution.index.name = \"\"\n",
    "display(val_graph_corr_type_distribution.iloc[9:18, ::])\n",
    "display(val_graph_corr_type_distribution.iloc[18:, ::])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be62fd-5335-40b2-a1de-a08cf0bd4998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-06T15:31:46.484282Z",
     "iopub.status.busy": "2023-09-06T15:31:46.483985Z",
     "iopub.status.idle": "2023-09-06T15:31:46.500401Z",
     "shell.execute_reply": "2023-09-06T15:31:46.499890Z",
     "shell.execute_reply.started": "2023-09-06T15:31:46.484257Z"
    },
    "tags": []
   },
   "source": [
    "## Observe which data in the validation dataset has been predicted incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e93f2-e38e-447c-8581-7d9d3f9f91fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b54b26c-8600-416e-b07d-bec52be981b9",
   "metadata": {},
   "source": [
    "## Observe the correctness ratio of each correlation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dad393-6925-4cf2-904d-8ab404d388f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_info_dict = {\"negative\": {\"mask\": (best_epoch_val_labels == 0),\n",
    "                              \"num\":(best_epoch_val_labels == 0).sum()},\n",
    "                 \"no_corr\": {\"mask\": (best_epoch_val_labels == 1),\n",
    "                             \"num\": (best_epoch_val_labels == 1).sum()},\n",
    "                 \"positive\": {\"mask\": (best_epoch_val_labels == 2),\n",
    "                              \"num\": (best_epoch_val_labels == 2).sum()}}\n",
    "pred_correct_mask = (best_epoch_val_preds == best_epoch_val_labels)\n",
    "assert sum([v[\"num\"] >= 0 for v in val_info_dict.values()]) == 3\n",
    "assert sum([v[\"num\"] for v in val_info_dict.values()]) == best_epoch_val_labels.size, \"the sum of mask should be same with all\"\n",
    "for corr_type, info in val_info_dict.items():\n",
    "    val_info_dict[corr_type][\"correct_ratio\"] = np.logical_and(info[\"mask\"], pred_correct_mask).sum()/info[\"num\"] if info[\"num\"] > 0 else None\n",
    "    logging.info(f\"corr_type:{corr_type}, num_corr_type:{val_info_dict[corr_type]['num']}, correct_ratio:{val_info_dict[corr_type]['correct_ratio']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
