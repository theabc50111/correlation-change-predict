{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2c178-aa69-4c4c-8ac1-90ba2563a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pformat, pprint\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from math import ceil\n",
    "from itertools import repeat, chain, product\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "logging.basicConfig(format='%(levelname)-8s [%(filename)s] %(message)s',\n",
    "                    level=logging.DEBUG)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bd00b-5f02-45ab-84f7-4f532d108249",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Draw the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55792e4-d68d-4b91-9626-816ab8d97d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mts_corr_ad_tr_proc_est(log_path_list: list, condition_dict: dict,  plot_pic:bool = True):\n",
    "    try:\n",
    "        df = pd.DataFrame()\n",
    "        for log_path in log_path_list:\n",
    "            with open(log_path, \"r\") as source:\n",
    "                log_dict = json.load(source)\n",
    "\n",
    "            corr_info = str(next(filter(lambda p: p.startswith(\"corr\"), log_path.parts)))\n",
    "            filt_mode = log_dict.get('filt_mode')\n",
    "            filt_quan = log_dict.get('filt_quan')\n",
    "            loss_fns = str(log_dict.get('loss_fns'))\n",
    "            discr_loss_r = log_dict.get('discr_loss_r')\n",
    "            discr_loss_disp_r = log_dict.get('discr_loss_disp_r')\n",
    "            drop_pos = str(log_dict.get('drop_pos'))\n",
    "            graph_enc = log_dict.get('graph_enc')\n",
    "            best_epoch_n = log_dict.get('best_val_epoch', 500)\n",
    "            tr_batch = log_dict.get('train_batch')\n",
    "            batchs_per_epoch_n = log_dict.get('batchs_per_epoch', 0)\n",
    "            loss_his_dict = {\"tr_loss\": log_dict.get('train_loss_history', [0]), \"tr_l2_loss\": log_dict.get('tr_l2_loss_history', [0]), \"tr_discr_loss\": log_dict.get('tr_discr_loss_history', [0]), \"val_loss\": log_dict.get('val_loss_history', [0])}\n",
    "            min_tr_loss, min_val_loss = min(loss_his_dict[\"tr_loss\"]), min(loss_his_dict[\"val_loss\"])\n",
    "            min_tr_l2_loss, min_tr_discr_loss = min(loss_his_dict[\"tr_l2_loss\"]), min(loss_his_dict[\"tr_discr_loss\"])\n",
    "\n",
    "            if model_struct_str := log_dict.get('model_structure'):\n",
    "                drop_p = re.search(\"\\(dropout\\): Dropout\\(p=(?P<drop_p>\\d*\\.\\d+|\\d+), inplace=False\\)\", model_struct_str).group('drop_p')\n",
    "                gra_enc_l = len(re.findall(\"\\(\\d\\)\\:\\s.*Conv\", model_struct_str))\n",
    "                gra_enc_h = int(re.search(\"(\\(\\d\\)\\:\\s.*Conv.*\\n.*)(out_features\\=)(\\d*)\", model_struct_str).group(3))\n",
    "                gru_l = int(re.search(\"(\\(gru1\\)\\:.*)(num_layers\\=)(\\d*)\", model_struct_str)[0][-1] if re.search(\"(\\(gru1\\)\\:.*)(num_layers\\=)(\\d*)\", model_struct_str) else 1)\n",
    "                gru_h = int(re.search(\"(\\(gru1\\)\\:\\sGRU\\(\\d*\\,)\\s(\\d*)\", model_struct_str).group(2))\n",
    "            else:\n",
    "                gra_enc_l, gra_enc_h, gru_l, gru_h = [None] * 4\n",
    "\n",
    "            if embeds_history := log_dict.get(\"graph_embeds_history\"):\n",
    "                pred_embeds_history = embeds_history.get('graph_embeds_pred', [0])\n",
    "                y_embeds_history = embeds_history.get('y_graph_embeds', [0])\n",
    "                tr_gra_enc_embeds_disp_history = embeds_history.get('graph_embeds_disparity', {\"train_gra_enc\": []}).get(\"train_gra_enc\", [])\n",
    "                val_gra_enc_embeds_disp_history = embeds_history.get('graph_embeds_disparity', {\"val_gra_enc\": []}).get(\"val_gra_enc\", [])\n",
    "                tr_pred_embeds_disp_history = embeds_history.get('graph_embeds_disparity', {\"train_pred\": []}).get(\"train_pred\", [])\n",
    "                val_pred_embeds_disp_history = embeds_history.get('graph_embeds_disparity', {\"val_pred\": []}).get(\"val_pred\", [])\n",
    "                embeds_his_dict = {\"pred_embeds\": np.array(pred_embeds_history[:batchs_per_epoch_n * 2]\\\n",
    "                                                               + [([np.nan] * (gra_enc_l * gra_enc_h)) for _ in range(20)]\\\n",
    "                                                               + pred_embeds_history[-batchs_per_epoch_n * 2:]),\n",
    "                                   \"y_embeds\": np.array(y_embeds_history[:batchs_per_epoch_n * 2]\\\n",
    "                                                        + [([np.nan] * (gra_enc_l * gra_enc_h)) for _ in range(20)]\\\n",
    "                                                        + y_embeds_history[-batchs_per_epoch_n * 2:]),\n",
    "                                   \"last_y_embeds\": y_embeds_history[-batchs_per_epoch_n * 5:],\n",
    "                                   \"tr_gra_enc_embeds_disp\": tr_gra_enc_embeds_disp_history,\n",
    "                                   \"val_gra_enc_embeds_disp\": val_gra_enc_embeds_disp_history,\n",
    "                                   \"tr_pred_embeds_disp\": tr_pred_embeds_disp_history,\n",
    "                                   \"val_pred_embeds_disp\": val_pred_embeds_disp_history}\n",
    "\n",
    "            assert not(set(condition_dict.keys()) - set(locals().keys())), \"one of condition_dict.keys() doesn't match the local variables if mts_corr_ad_est()\"\n",
    "            est_values_dict = locals()\n",
    "            filtered_dict = dict(filter(lambda x: est_values_dict[x[0]] == x[1], condition_dict.items()))\n",
    "            if filtered_dict == condition_dict:\n",
    "                main_title_str = f'{corr_info} with filt:{filt_mode}-{filt_quan} and tr_batch({tr_batch}) input to {graph_enc} with gra_enc_l{gra_enc_l}-gra_enc_h{gra_enc_h}-gru_l{gru_l}-gru_h{gru_h}\\nwith drop:{drop_pos}-{drop_p} and loss_fn:{loss_fns}\\n min val-loss:{min_val_loss:8f}'\n",
    "                logging.info(f\"file_name:{log_path.parts[-1]}\")\n",
    "                logging.info(f\"file_path:{log_path.parts[2:-2]}\")\n",
    "                logging.info(main_title_str)\n",
    "                comparison_dict = dict(filter(lambda x: x[0] in [ \"corr_info\", \"tr_batch\", \"filt_mode\", \"filt_quan\", \"loss_fns\", \"discr_loss_r\", \"discr_loss_disp_r\", \"drop_pos\", \"drop_p\", \"graph_enc\", \"gra_enc_l\", \"gra_enc_h\", \"gru_l\", \"gru_h\", \"min_tr_loss\", \"min_tr_l2_loss\", \"min_tr_discr_loss\", \"min_val_loss\"], locals().items()))\n",
    "                df = pd.concat([df, pd.DataFrame([comparison_dict])])\n",
    "                if plot_pic:\n",
    "                    plot_mts_corr_ad_tr_process(main_title=main_title_str, model_struct=model_struct_str,\n",
    "                                                loss_history=loss_his_dict, embeds_history=embeds_his_dict,\n",
    "                                                best_epoch=best_epoch_n, batchs_per_epoch=batchs_per_epoch_n)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            df = df.reindex([\"corr_info\", \"tr_batch\", \"filt_mode\", \"filt_quan\", \"loss_fns\", \"discr_loss_r\", \"discr_loss_disp_r\", \"drop_pos\", \"drop_p\", \"graph_enc\", \"gra_enc_l\", \"gra_enc_h\", \"gru_l\", \"gru_h\", \"min_tr_loss\", \"min_tr_l2_loss\", \"min_tr_discr_loss\", \"min_val_loss\"], axis=1)\n",
    "            df = df.sort_values([\"tr_batch\", \"gra_enc_l\", \"gra_enc_h\", \"gru_l\", \"gru_h\", \"filt_mode\", \"filt_quan\", \"graph_enc\", \"loss_fns\", \"drop_pos\", \"discr_loss_r\", \"discr_loss_disp_r\", \"drop_p\"], ascending=False)\n",
    "            df = df.reset_index(drop=True)\n",
    "            df = df.style.set_caption('Info of MTSCorrAD model with different hyperparameters')\n",
    "            display(df)\n",
    "    except Exception as e:\n",
    "        error_class = e.__class__.__name__ #⬞取得錯誤類型\n",
    "        detail = e.args[0]  #⬞取得詳細內容\n",
    "        cl, exc, tb = sys.exc_info() #⬞取得Call⬞Stack\n",
    "        last_call_stack = traceback.extract_tb(tb)[-1] #⬞取得Call⬞Stack的最後一筆資料↵\n",
    "        file_name = last_call_stack[0] #⬞取得發生的檔案名稱↵\n",
    "        line_num = last_call_stack[1] #⬞取得發生的行號↵\n",
    "        func_name = last_call_stack[2] #⬞取得發生的函數名稱\n",
    "        err_msg = \"File \\\"{}\\\", line {}, in {}: [{}] {}\".format(file_name, line_num, func_name, error_class, detail)\n",
    "        logging.error(f\"file:{log_path.parts[-1]}, path:{log_path}\")\n",
    "        logging.error(f\"===\\n{err_msg}\")\n",
    "        logging.error(f\"===\\n{traceback.extract_tb(tb)}\")\n",
    "\n",
    "\n",
    "def plot_mts_corr_ad_tr_process(main_title: str, model_struct: str, loss_history: dict, embeds_history: dict, best_epoch: int, batchs_per_epoch: int):\n",
    "    pred_embeds, y_embeds, last_y_embeds = embeds_history['pred_embeds'], embeds_history['y_embeds'], embeds_history['last_y_embeds']\n",
    "    tr_gra_enc_embeds_disp_dict, val_gra_enc_embeds_disp_dict = embeds_history[\"tr_gra_enc_embeds_disp\"], embeds_history[\"val_gra_enc_embeds_disp\"]\n",
    "    tr_pred_embeds_disp_dict, val_pred_embeds_disp_dict = embeds_history[\"tr_pred_embeds_disp\"], embeds_history[\"val_pred_embeds_disp\"]\n",
    "    max_batch = batchs_per_epoch * len(loss_history['tr_loss'])  # epochs == len(loss_history['tr_loss'])\n",
    "    xticks_intv = {\"loss\": 20,\n",
    "                   \"fr_ls_embeds\": int(len(y_embeds)/10)}\n",
    "    loss_xticks_label = list(range(max(0, best_epoch-100), max(201, best_epoch+101), xticks_intv[\"loss\"]))\n",
    "    fr_ls_embeds_xticks_label = list(range(0, batchs_per_epoch*2, xticks_intv[\"fr_ls_embeds\"])) + [\" \"] + list(range(max_batch-xticks_intv[\"fr_ls_embeds\"]*4, max_batch+1, xticks_intv[\"fr_ls_embeds\"]))\n",
    "    data_info_dict = [{\"sub_title\": 'train_loss_history',\n",
    "                       \"data\": loss_history['tr_loss'],\n",
    "                       \"xticks\": None,\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": 'val_loss_history',\n",
    "                       \"data\": loss_history['val_loss'],\n",
    "                       \"xticks\": None,\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": f\"train_loss_history-epoch{(max(0, best_epoch-100), max(200, best_epoch+100))}\",\n",
    "                       \"data\": loss_history['tr_loss'][max(0, best_epoch-100):max(201, best_epoch+101)],\n",
    "                       \"xticks\": {\"label\": loss_xticks_label, \"intv\": xticks_intv['loss']},\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": f\"val_loss_history-epoch{(max(0, best_epoch-100), max(200, best_epoch+100))}\",\n",
    "                       \"data\": loss_history['val_loss'][max(0, best_epoch-100):max(201, best_epoch+101)],\n",
    "                       \"xticks\": {\"label\": loss_xticks_label, \"intv\": xticks_intv['loss']},\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": \"train, train_l2, train_discrimination loss & valdation loss\",\n",
    "                       \"data\": []},\n",
    "                      {\"sub_title\": f\"First train graph encoder embeds disparity\",\n",
    "                       \"data\": tr_gra_enc_embeds_disp_dict,\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": f\"First train predition embeds disparity\",\n",
    "                       \"data\": tr_pred_embeds_disp_dict,\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": f\"First val graph encoder embeds disparity\",\n",
    "                       \"data\": val_gra_enc_embeds_disp_dict,\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": f\"First val predition embeds disparity\",\n",
    "                       \"data\": val_pred_embeds_disp_dict,\n",
    "                       \"xlabel\": \"epochs\"},\n",
    "                      {\"sub_title\": f'pred_embeds, embeds size:[{pred_embeds.shape[1]}]',\n",
    "                       \"data\": pred_embeds,\n",
    "                       \"xticks\": {\"label\": fr_ls_embeds_xticks_label, \"intv\": xticks_intv[\"fr_ls_embeds\"]},\n",
    "                       \"xlabel\": \"batchs\",\n",
    "                       \"axvline\": (batchs_per_epoch, batchs_per_epoch*3+20)},\n",
    "                      {\"sub_title\": f'y_embeds, embeds size:[{y_embeds.shape[1]}]',\n",
    "                       \"data\": y_embeds,\n",
    "                       \"xticks\": {\"label\": fr_ls_embeds_xticks_label, \"intv\": xticks_intv[\"fr_ls_embeds\"]},\n",
    "                       \"xlabel\": \"batchs\",\n",
    "                       \"axvline\": (batchs_per_epoch, batchs_per_epoch*3+20)},\n",
    "                      {\"sub_title\": f\"y_embeds in last five epochs; embeds size:{y_embeds.shape[1]}\",\n",
    "                       \"data\": last_y_embeds,\n",
    "                       \"xticks\": {\"label\": range(max_batch - batchs_per_epoch * 5, max_batch + 1, batchs_per_epoch), \"intv\": batchs_per_epoch},\n",
    "                       \"xlabel\": \"batchs\",\n",
    "                       \"axvline\": [i*batchs_per_epoch for i in range(1, 5)]},\n",
    "                      {\"sub_title\": f\"model structure\",\n",
    "                       \"data\": str(model_struct)}]\n",
    "\n",
    "    # figrue settings\n",
    "    line_style = {\"linewidth\": 2, \"alpha\": 0.5}\n",
    "    axvline_style = {\"color\": 'k', \"linewidth\": 5, \"linestyle\": '--', \"alpha\": 0.3}\n",
    "    nrows, ncols = 9, 2  # define the number of rows and columns for the plot\n",
    "    gs = gridspec.GridSpec(nrows, ncols)  # create a GridSpec object with the desired layout\n",
    "    fig = plt.figure(figsize=(25, 90))  # create a figure with the subplots specified by the GridSpec object\n",
    "    fig.suptitle(main_title, fontsize=30)\n",
    "    axes = []\n",
    "    for nrow, ncol in product(range(nrows), range(ncols)):\n",
    "        if nrow <= 4:\n",
    "            ax = fig.add_subplot(gs[nrow, ncol])\n",
    "        elif 4 < nrow < 6 and ncol == 0:\n",
    "            ax = fig.add_subplot(gs[nrow, :2])\n",
    "        elif nrow == 6 and ncol == 0:\n",
    "            ax = fig.add_subplot(gs[nrow:, :2])\n",
    "        else:\n",
    "            continue\n",
    "        axes.append(ax)\n",
    "\n",
    "    try:\n",
    "        for ax, data_plot in zip(axes, data_info_dict):\n",
    "            ax.set_title(data_plot[\"sub_title\"], fontsize=30)\n",
    "            ax.yaxis.offsetText.set_fontsize(18)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "            if isinstance(data_plot[\"data\"], dict):\n",
    "                [ax.plot(data_plot[\"data\"][key], label=key) for key in data_plot[\"data\"]]\n",
    "                ax.legend(fontsize=18)\n",
    "            elif isinstance(data_plot[\"data\"], str):\n",
    "                ax.annotate(text=f\"{data_plot['data']}\",\n",
    "                            xy=(0.15, 0.5), bbox={'facecolor': 'green', 'alpha': 0.4, 'pad': 5},\n",
    "                            fontsize=20, fontfamily='monospace', xycoords='axes fraction', va='center')\n",
    "            else:\n",
    "                ax.plot(data_plot[\"data\"], **line_style)\n",
    "            if pos_tuple := data_plot.get(\"axvline\"):\n",
    "                for x_pos in pos_tuple:\n",
    "                    ax.axvline(x=x_pos, **axvline_style)\n",
    "            if xlabel := data_plot.get(\"xlabel\"):\n",
    "                ax.set_xlabel(xlabel, fontsize=24)\n",
    "            if t := data_plot.get(\"xticks\"):\n",
    "                ax.set_xticks(ticks=range(0, len(t[\"label\"])*t[\"intv\"], t[\"intv\"]), labels=t[\"label\"], rotation=45)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Encounter error when draw figure of {data_plot['sub_title']}\")\n",
    "        raise e\n",
    "\n",
    "    fig.tight_layout(rect=(0, 0, 1, 0.97))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87825fad-c2a3-4c17-af2d-7aa3edc0bf35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mts_corr_model_log_dir = Path(\"./save_models/sp500_20082017_corr_ser_reg_corr_mat_hrchy_11_cluster-train_train/\")\n",
    "log_path_list1 = mts_corr_model_log_dir.glob(\"./*[!deprecated][!archive][!.ipynb_checkpoints]*/train_logs/*[!.ipynb_checkpoints]*[.json]\")\n",
    "log_path_list2 = mts_corr_model_log_dir.glob(\"./*[archive][!deprecated][!.ipynb_checkpoints]*/**/train_logs/*[!.ipynb_checkpoints]*[.json]\")\n",
    "log_path_list3 = mts_corr_model_log_dir.glob(\"./**/train_logs/*[!.ipynb_checkpoints]*[.json]\")\n",
    "\n",
    "# mts_corr_ad_tr_proc_est(log_path_list1, {\"corr_info\": \"corr_s1_w10\", \"tr_batch\": 32, \"gra_enc_l\": 1, \"gra_enc_h\": 4, \"gru_l\": 1, \"gru_h\": 8})\n",
    "# mts_corr_ad_tr_proc_est(log_path_list1, {\"corr_info\": \"corr_s1_w10\", \"gra_enc_l\": 5, \"gru_l\": 1, \"gru_h\": 8})\n",
    "# mts_corr_ad_tr_proc_est(log_path_list1, {\"corr_info\": \"corr_s1_w10\", \"gra_enc_l\": 5, \"gra_enc_h\": 16, \"filt_mode\": \"keep_strong\", \"graph_enc\":\"GineEncoder\"}, plot_pic=False)\n",
    "mts_corr_ad_tr_proc_est(log_path_list1, {\"corr_info\": \"corr_s1_w10\"}, plot_pic=False)\n",
    "# mts_corr_ad_tr_proc_est(log_path_list1, {\"corr_info\": \"corr_s1_w10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bfee75-07a0-4224-8f20-0bd1a70cd1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_log_p = mts_corr_model_log_dir/f\"corr_s1_w10/train_logs/epoch_251-20230305215527.json\"\n",
    "\n",
    "with open(specific_log_p, \"r\") as source:\n",
    "    log_dict = json.load(source)\n",
    "\n",
    "if log_dict.get('model_structure'):\n",
    "    gin_l = len(re.findall(\"\\(\\d\\)GINConv\", log_dict.get('model_structure')))\n",
    "    gin_h = int(re.search(\"(\\(\\d\\)\\:\\sGINConv.*\\n.*)(out_features\\=)(\\d*)\", log_dict.get('model_structure')).group(3))\n",
    "    gru_l = int(re.search(\"(\\(gru1\\)\\:.*)(num_layers\\=)(\\d*)\", log_dict.get('model_structure'))[0][-1] if re.search(\"(\\(gru1\\)\\:.*)(num_layers\\=)(\\d*)\", log_dict.get('model_structure')) else 1)\n",
    "    gru_h = int(re.search(\"(\\(gru1\\)\\:\\sGRU\\(\\d*\\,)\\s(\\d*)\", log_dict.get('model_structure')).group(2))\n",
    "else:\n",
    "    gin_l = None\n",
    "    gin_h = None\n",
    "    gru_l = None\n",
    "    gru_h = None\n",
    "corr_info = [p for p in specific_log_p.parts if p.startswith(\"corr\")][0]\n",
    "best_epoch = log_dict['best_val_epoch'] if log_dict.get('best_val_epoch') else 500\n",
    "min_val_loss = min(log_dict['val_loss_history'])\n",
    "tr_batch = log_dict.get('train_batch') if log_dict.get('train_batch') else None\n",
    "batchs_per_epoch = log_dict.get('batchs_per_epoch')\n",
    "tr_loss = log_dict.get('train_loss_history')\n",
    "val_loss = log_dict.get('val_loss_history')\n",
    "pred_embeds = np.array(log_dict.get('graph_embeds_history').get('graph_embeds_pred')[:batchs_per_epoch*2]\\\n",
    "                       + [([np.nan]*(gin_l*gin_h)) for _ in range(20)]\\\n",
    "                       + log_dict.get('graph_embeds_history').get('graph_embeds_pred')[-batchs_per_epoch*2:])\n",
    "y_embeds = np.array(log_dict.get('graph_embeds_history').get('y_graph_embeds')[:batchs_per_epoch*2]\\\n",
    "                    + [([np.nan]*(gin_l*gin_h)) for _ in range(20)]\\\n",
    "                    + log_dict.get('graph_embeds_history').get('y_graph_embeds')[-batchs_per_epoch*2:])\n",
    "plt.figure(figsize=(14.5, 8))\n",
    "plt.plot(y_embeds, linewidth=5, alpha=0.3)\n",
    "plt.axvline(x=batchs_per_epoch, ymin=y_embeds[~np.isnan(y_embeds)].min(), ymax=y_embeds[~np.isnan(y_embeds)].max(),\n",
    "                  color='k', linewidth=5, linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=batchs_per_epoch*3+20, ymin=y_embeds[~np.isnan(y_embeds)].min(), ymax=y_embeds[~np.isnan(y_embeds)].max(),\n",
    "                  color='k', linewidth=5, linestyle='--', alpha=0.3)\n",
    "plt.annotate(text=f\"188\", xy=(0.19, 0.5),\n",
    "             bbox={'facecolor': 'gray', 'alpha': 0.4, 'pad': 5},\n",
    "             fontsize=20, fontfamily='monospace', xycoords='axes fraction', va='center')\n",
    "plt.annotate(text=f\"187811\", xy=(0.77, 0.5),\n",
    "             bbox={'facecolor': 'gray', 'alpha': 0.4, 'pad': 5},\n",
    "             fontsize=20, fontfamily='monospace', xycoords='axes fraction', va='center')\n",
    "plt.title(f'y_embeds-[{y_embeds.shape[1]}]', fontsize=30)\n",
    "xticks_label = list(range(0, 301, 100)) + list(range(187600, 188001, 100))\n",
    "plt.xticks(ticks=list(range(0, 801, 100)), labels=xticks_label, fontsize=18)\n",
    "plt.yticks(fontsize=24)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1aa30-d4d5-498b-b39d-1a277260faee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Find the most differ graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cba67d-92de-4d4f-9fad-89660ae90268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/workspace/correlation-change-predict/ywt_library\")\n",
    "current_dir = Path(__file__).parent\n",
    "data_config_path = current_dir/\"../config/data_config.yaml\"\n",
    "with open(data_config_path) as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "# ## Data implement & output setting & testset setting\n",
    "# data implement setting\n",
    "data_implement = \"SP500_20082017_CORR_SER_REG_CORR_MAT_HRCHY_11_CLUSTER\"  # watch options by operate: logging.info(data_cfg[\"DATASETS\"].keys())\n",
    "# train set setting\n",
    "train_items_setting = \"-train_train\"  # -train_train|-train_all\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + train_items_setting\n",
    "# setting of output files\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")\n",
    "graph_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-graph_data\"\n",
    "graph_arr = np.load(graph_data_dir/f\"corr_s1_w10_graph.npy\")  # each graph consist of 66 node & 66^2 edges\n",
    "\n",
    "stride = 12\n",
    "train_arr = graph_arr[:int(len(graph_arr)*0.9)]\n",
    "val_arr = graph_arr[int(len(graph_arr)*0.9):int(len(graph_arr)*0.95)]\n",
    "test_arr = graph_arr[int(len(graph_arr)*0.95):]\n",
    "train_diff_arr = train_arr[stride:] - train_arr[:-stride] # this is what I want\n",
    "max_diff_ind = np.argmax(train_diff_arr.sum(axis=1).sum(axis=1))\n",
    "logging.info(f\"train_arr.shape: {train_arr.shape}\")\n",
    "logging.info(f\"train_diff_arr.shape: {train_diff_arr.shape}\")\n",
    "logging.info(f\"train_arr[0][0][:5]: \\n{train_arr[0][0][:5]}\")\n",
    "logging.info(f\"max_difference index of train_arr: {max_diff_ind}\")\n",
    "logging.info(f\"train_diff_arr[{max_diff_ind}][0]: \\n{train_diff_arr[max_diff_ind][0]}\")\n",
    "logging.info(f\"train_arr[{max_diff_ind}][0]: \\n{train_arr[max_diff_ind][0]}\")\n",
    "logging.info(f\"train_arr[{max_diff_ind+stride}][0]: \\n{train_arr[max_diff_ind+stride][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
